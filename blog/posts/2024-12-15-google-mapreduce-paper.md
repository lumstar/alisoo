---
title: "MapReduce: 超大机群上的简单数据处理"
date: "2024-12-15"
category: "大数据"
tags: ["MapReduce", "Google", "分布式计算", "大数据", "分布式系统"]
excerpt: "MapReduce是Google提出的一个编程模型，也是一个处理和生成超大数据集的算法模型的相关实现。本文是MapReduce经典论文的中文翻译版本。"
author: "Google Research"
translator: "小熊餐馆"
---

# MapReduce: 超大机群上的简单数据处理

**原文标题**: MapReduce: Simplified Data Processing on Large Clusters  
**作者**: Jeffrey Dean and Sanjay Ghemawat  
**发表**: OSDI 2004  
**原文链接**: https://research.google.com/archive/mapreduce-osdi04.pdf

## 摘要

MapReduce是一个编程模型，也是一个处理和生成超大数据集的算法模型的相关实现。用户首先创建一个Map函数处理一个基于key/value pair的数据集合，输出中间的基于key/value pair的数据集合；然后再创建一个Reduce函数用来合并所有的具有相同中间key值的中间value值。现实世界中有很多满足这种处理模型的例子，本论文将展示这种模型的细节。

这种函数式风格的程序会被自动并行化执行在一个由普通机器组成的大集群上。这个运行时系统的职责是：处理数据分区、调度程序在机器集群中的执行、处理机器的失效、管理机器之间必要的通信。这让那些没有并行计算和分布式处理系统经验的程序员也能够轻易地利用一个大型分布式系统的资源。

我们的MapReduce实现运行在规模可以灵活调整的由普通机器组成的集群上：一个典型的MapReduce计算往往由几千台机器组成、处理以TB计算的数据。程序员发现这个系统非常好用：已经实现了数以百计的MapReduce程序，每天在Google的集群上都有1000多个MapReduce程序在执行。

## 1 引言

在过去的5年里，包括本文作者在内的Google的很多程序员，为了处理海量的原始数据，已经实现了数以百计的、专用的计算方法。这些计算方法用来处理大量的原始数据，比如，文档抓取（类似网页爬虫的程序）、Web请求日志等等；也为了计算处理各种类型的衍生数据，比如倒排索引、Web文档的图结构的各种表示形势、每台主机上网页的数量、每天被请求的最多的查询的集合等等。大多数这样的运算在概念上很容易理解。然而由于输入的数据量巨大，因此要想在可接受的时间内完成运算，只有将这些计算分布在成百上千的机器上。如何处理并行计算、如何分发数据、如何处理错误？所有这些问题综合在一起，需要大量的代码处理，因此也使得原本简单的运算变得难以处理。

为了解决上述复杂的问题，我们设计一个新的抽象模型，使用这个抽象模型，我们只要表述我们想要执行的简单运算即可，而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，这些问题都被封装在了一个库里面。设计这个抽象模型的灵感来源于Lisp和许多其他函数式语言里的map和reduce的原语。我们意识到我们大多数的运算都包含这样的操作：在输入数据的"逻辑"记录上应用Map操作得出一个中间key/value pair集合，然后在所有具有相同key值的value值上应用Reduce操作，从而达到合并中间的数据，得到一个想要的结果的目的。使用MapReduce模型，再结合用户实现的Map和Reduce函数，我们就可以非常容易的实现大规模并行化计算；通过MapReduce模型自带的重新执行功能，也提供了初级的容灾实现。

这个工作(实现一个MapReduce框架库)的主要贡献是通过简单的接口来实现自动的并行化和大规模的分布式计算，通过使用MapReduce模型接口实现在大量普通的PC上高性能计算。

第二部分描述基本的编程模型和一些使用案例。第三部分描述了一个经过裁剪的、适合我们的基于集群的计算环境的MapReduce实现。第四部分描述我们认为在MapReduce编程模型中一些有用的技巧。第五部分对于各种不同的任务，测试了我们MapReduce实现的性能。第六部分揭示了在Google内部如何使用MapReduce作为基础重写我们的索引系统产品，包括其它一些使用MapReduce的经验。第七部分讨论相关的和未来的工作。

## 2 编程模型

MapReduce编程模型的原理是：利用一个输入key/value pair集合来产生一个输出的key/value pair集合。MapReduce库的用户用两个函数表达这个计算：Map和Reduce。

用户自定义的Map函数接受一个输入的key/value pair值，然后产生一个中间key/value pair值的集合。MapReduce库把所有具有相同中间key值I的中间value值集合在一起后传递给reduce函数。

用户自定义的Reduce函数接受一个中间key的值I和相关的一个value值的集合。Reduce函数合并这些value值，形成一个较小的value值的集合。一般的，每次Reduce函数调用只产生0或1个输出value值。通常我们通过一个迭代器把中间value值提供给Reduce函数，这样我们就可以处理无法全部放入内存中的大量的value值的集合。

### 2.1 例子

考虑这样一个问题：计算一个大的文档集合中每个单词出现的次数，下面是伪代码段：

```
map(String key, String value):
    // key: 文档名称
    // value: 文档内容
    for each word w in value:
        EmitIntermediate(w, "1");

reduce(String key, Iterator values):
    // key: 单词
    // values: 计数列表
    int result = 0;
    for each v in values:
        result += ParseInt(v);
    Emit(AsString(result));
```

Map函数输出文档中的每个词、以及这个词的出现次数(在这个简单的例子里就是1)。Reduce函数把Map函数产生的每一个特定的词的计数累加起来。

另外，用户编写代码，使用输入和输出文件的名字、以及可选的调优参数来完成一个符合MapReduce模型规范的对象，然后调用MapReduce函数，并把这个规范对象传递给它。用户的代码和MapReduce库链接在一起(用C++实现)。附录A包含了这个实例的全部程序代码。

### 2.2 类型

尽管前面的伪代码是以字符串类型的输入和输出为基础的，但是，从概念上讲，用户提供的map和reduce函数都有相关联的类型：

```
map (k1,v1) → list(k2,v2)
reduce (k2,list(v2)) → list(v2)
```

比如，输入的key和value值与输出的key和value值在类型上可以不同。此外，中间的key和value值与输出key和value值的类型必须相同。

我们的C++中使用字符串类型作为用户自定义函数的输入和输出，用户在自己的代码中对字符串进行适当的类型转换。

### 2.3 更多的例子

这里还有一些有趣的简单例子，可以很容易的使用MapReduce模型来表示：

**分布式的Grep**：Map函数输出匹配某个模式的一行，Reduce函数是一个恒等函数，即把中间数据复制到输出。

**计算URL访问频率**：Map函数处理日志中web页面请求的记录，然后输出(URL,1)。Reduce函数把相同URL的value值都累加起来，产生(URL,记录总数)结果。

**倒转网络链接图**：Map函数在源页面(source)中找到所有的链接目标(target)并输出为(target,source)。Reduce函数把给定链接目标(target)的链接源(source)合并成一个列表，输出(target,list(source))。

**每个主机的检索词向量**：检索词向量用一个(词,频率)列表来概括出现在文档或文档集中的最重要的一些词。Map函数为每一个输入文档输出(主机名,检索词向量)，其中主机名来自文档的URL。Reduce函数接收给定主机的所有文档的检索词向量，并把这些检索词向量加在一起，丢弃掉低频的检索词，然后产生一个最终的(主机名,检索词向量)。

**倒排索引**：Map函数分析每个文档输出一个(词,文档号)的列表，Reduce函数的输入是一个给定词的所有(词,文档号)，排序所有的文档号，输出(词,list(文档号))。所有的输出集合形成一个简单的倒排索引，它以一种简单的算法跟踪词在文档中的位置。

**分布式排序**：Map函数从每个记录提取key，输出(key,record)。Reduce函数不改变任何的值，这种运算依赖分区机制(在4.1描述)和排序属性(在4.2描述)。

## 3 实现

MapReduce接口的实现可以有多种不同的方式，如何正确选择取决于具体的环境。例如，一种实现方式适用于小型的共享内存方式的机器，另外一种实现方式则适用于大型NUMA架构的多处理器的主机，而有的实现方式更适合大型的网络连接集群。

本章节描述一个适用于Google内部广泛使用的运算环境的实现：用以太网交换机连接、由普通PC机组成的大型集群。在我们的环境里包括：

(1) x86架构、运行Linux操作系统、双处理器、2-4GB内存的机器。

(2) 普通的网络硬件设备，每个机器的带宽为百兆或者千兆，但是远小于网络的平均带宽的一半。

(3) 集群中包含成百上千的机器，因此，机器故障是常态。

(4) 存储为廉价的内置IDE硬盘。一个内部分布式文件系统来管理存储在这些磁盘上的数据。文件系统通过数据复制来在不可靠的硬件上保证数据的可靠性和有效性。

(5) 用户提交工作(job)给调度系统。每个工作(job)都包含一系列的任务(task)，调度系统将这些任务调度到集群中多台可用的机器上。

### 3.1 执行概况

通过将Map调用的输入数据自动分割为M个数据片段的集合，Map调用被分布到多台机器上执行。输入的数据片段能够在不同的机器上并行处理。使用分区函数将Map调用产生的中间key值分成R个不同分区(例如，hash(key) mod R)，Reduce调用也被分布到多台机器上执行。分区数量(R)和分区函数由用户来指定。

图1展示了我们的MapReduce实现中操作的全部流程。当用户调用MapReduce函数时，将发生下面的一系列动作(下面的序号和图1中的序号一一对应)：

1. 用户程序首先调用的MapReduce库将输入文件分成M个数据片度，每个数据片段的大小一般从16MB到64MB(可以通过可选的参数来控制每个数据片段的大小)。然后用户程序在机群中创建大量的程序副本。

2. 这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是worker程序，由master分配任务。有M个Map任务和R个Reduce任务将被分配，master将一个Map任务或Reduce任务分配给一个空闲的worker。

3. 被分配了map任务的worker程序读取相关的输入数据片段，从输入的数据片段中解析出key/value pair，然后把key/value pair传递给用户自定义的Map函数，由Map函数生成并输出的中间key/value pair，并缓存在内存中。

4. 缓存中的key/value pair通过分区函数分成R个区域，之后周期性的写入到本地磁盘上。缓存的key/value pair在本地磁盘上的存储位置将被回传给master，由master负责把这些存储位置再传送给Reduce worker。

5. 当Reduce worker程序接收到master程序发来的数据存储位置信息后，使用RPC从Map worker所在主机的磁盘上读取这些缓存数据。当Reduce worker读取了所有的中间数据后，通过对key进行排序后使得具有相同key值的数据聚合在一起。由于许多不同的key值会映射到相同的Reduce任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。

6. Reduce worker程序遍历排序后的中间数据，对于每一个唯一的中间key值，Reduce worker程序将这个key值和它相关的中间value值的集合传递给用户自定义的Reduce函数。Reduce函数的输出被追加到所属分区的输出文件。

7. 当所有的Map和Reduce任务都完成之后，master唤醒用户程序。在这个时候，在用户程序里的对MapReduce调用才返回。

在成功完成任务之后，MapReduce的输出存放在R个输出文件中(对应每个Reduce任务产生一个输出文件，文件名由用户指定)。一般情况下，用户不需要将这R个输出文件合并成一个文件–他们经常把这些文件作为另外一个MapReduce的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用。

### 3.2 Master数据结构

Master持有一些数据结构，它存储每一个Map和Reduce任务的状态（空闲、工作中或完成），以及Worker机器(非空闲任务的机器)的标识。

Master就像一个数据管道，中间文件存储区域的位置信息通过这个管道从Map任务传递到Reduce任务。因此，对于每个已经完成的Map任务，master存储了由Map任务产生的R个中间文件存储区域的大小和位置。当Map任务完成时，Master接收到位置和大小的更新信息，这些信息被逐步递增的推送给那些正在工作的Reduce任务。

### 3.3 容错

因为MapReduce库的设计初衷是使用由成百上千的机器组成的集群来处理超大规模的数据，所以，这个库必须要能够很好的处理机器故障。

**Worker故障**

master周期性的ping每个worker。如果在一个约定的时间范围内没有收到worker返回的信息，master将把这个worker标记为失效。所有由这个失效的worker完成的Map任务被重设为初始的空闲状态，之后这些任务就可以被安排给其他的worker。同样的，失效worker正在运行的Map或Reduce任务也将被重新设置为空闲状态，等待重新调度。

当worker故障时，由于已经完成的Map任务的输出存储在这台机器的本地磁盘上，Map任务的输出已不可访问了，因此必须重新执行。而已经完成的Reduce任务的输出存储在全局文件系统上，因此不需要再次执行。

当一个Map任务首先被worker A执行，之后由于worker A失效了又被调度到worker B执行，这个"重新执行"的通知会被发送给所有执行Reduce任务的worker。任何还没有从worker A读取数据的Reduce任务将从worker B读取数据。

MapReduce可以处理大规模worker失效的情况。比如，在一个MapReduce操作执行期间，在正在运行的集群上进行网络维护引起80台机器在几分钟内不可访问了，MapReduce master只需要简单的再次执行那些不可访问的worker机器完成的工作，之后继续执行未完成的任务，直到最终完成这个MapReduce操作。

**Master故障**

一个简单的解决办法是让master周期性的将上面描述的数据结构写入磁盘，即检查点(checkpoint)。如果这个master任务失效了，可以从最后一个检查点(checkpoint)开始启动另一个master进程。然而，由于只有一个master进程，master失效后再恢复是比较麻烦的；因此我们现在的实现是如果master失效，就中止MapReduce运算。客户可以检查到这个状态，并且可以根据需要重新执行MapReduce操作。

**在失效方面的处理机制**

当用户提供的Map和Reduce操作是输入确定性函数(即相同的输入产生相同的输出)时，我们的分布式实现在任何情况下的输出都和所有程序没有出现任何错误、顺序的执行产生的输出是一样的。

我们依赖对Map和Reduce任务的输出是原子提交的来完成这个特性。每个工作中的任务把它的输出写到私有的临时文件中。每个Reduce任务产生一个这样的文件，而每个Map任务则产生R个这样的文件(每个Reduce任务对应一个文件)。当一个Map任务完成的时，worker发送一个包含R个临时文件名的完成消息给master。如果master从一个已经完成的Map任务再次接收到到一个完成消息，master将忽略这个消息；否则，master将这R个文件的名字记录在数据结构里。

当Reduce任务完成时，Reduce worker以原子的方式把临时文件重命名为最终的输出文件。如果同一个Reduce任务在多台机器上执行，针对同一个最终的输出文件将有多个重命名操作执行。我们依赖底层文件系统提供的重命名操作的原子性来保证最终的文件系统状态仅仅包含一个Reduce任务产生的数据。

我们的Map和Reduce操作的绝大多数都是确定性的，而且我们的处理机制等价于一个顺序的执行的处理机制，这个特性使得程序员很容易分析他们程序的行为。当Map或者Reduce操作是非确定性的时候，我们提供虽然较弱但是依然合理的处理机制。当使用非确定性操作的时候，一个Reduce任务R1的输出等价于一个非确定性程序顺序执行产生时的输出。但是，另一个Reduce任务R2的输出也许符合一个不同的非确定性程序顺序执行产生的R2的输出。

考虑一个Map任务M和两个Reduce任务R1、R2的情况。设e(Ri)是Ri已经提交的执行过程(这里是指执行任务Ri的那个worker执行任务成功，master已经确认该worker执行成功这个时间点之前的执行过程)。那么较弱的处理机制保证e(R1)和e(R2)都是串行执行的整个程序产生的输出。

### 3.4 存储位置

在我们的计算运行环境中，网络带宽是一个相当匮乏的资源。我们通过尽量把输入数据(由GFS管理)存储在集群中机器的本地磁盘上来节省网络带宽。GFS把每个文件按64MB一个Block分隔，每个Block保存在多台机器上，环境中就存放了多份拷贝(一般是3个拷贝)。MapReduce的master在调度Map任务时会考虑输入文件的位置信息，尽量将一个Map任务调度在包含相关输入数据拷贝的机器上执行；如果上述努力失败了，master将尝试在保存有输入数据拷贝的机器附近的机器上执行Map任务(例如，分配到一个和包含输入数据的机器在一个switch里的worker机器上执行)。当在一个足够大的cluster集群上运行大型MapReduce操作的时候，大部分的输入数据都能从本地机器读取，因此消耗非常少的网络带宽。

### 3.5 任务粒度

如前所述，我们把Map拆分成了M个片段、把Reduce拆分成R个片段执行。理想情况下，M和R应当比集群中worker的机器数量要多得多。在每台worker机器都执行大量的不同任务能够提高集群的动态的负载均衡能力，并且能够加快故障恢复的速度：失效机器上执行的大量Map任务都可以分布到所有其他的worker机器上去执行。

但是实际上，在我们的具体实现中对M和R的取值都有一定的客观限制，因为master必须执行O(M+R)次调度，并且在内存中保存O(M*R)个状态(对影响内存使用的因素还是比较小的：O(M*R)块状态，大概每一块1个字节)。

更进一步，R值通常是由用户指定的，因为每个Reduce任务最终都会产生一个独立的输出文件。实际使用时我们也倾向于选择合适的M值，以使得每一个独立任务都是处理大约16M到64M的输入数据(这样，上面描述的输入数据本地存储优化策略才最有效)，另外，我们把R值设置为我们想要使用的worker机器数量的小的倍数。我们通常会用这样的比例来执行MapReduce：M=200000，R=5000，使用2000台worker机器。

### 3.6 备用任务

影响MapReduce的总执行时间最通常的因素是"落伍者"：在运算过程中，如果有一台机器花了很长的时间才完成最后几个Map或Reduce任务，导致MapReduce操作总的执行时间超过预期。出现"落伍者"的原因很多。比如：如果一个机器的硬盘出了问题，在读取的时候要经常的进行读取纠错操作，导致读取数据的速度从30M/s降低到1M/s。如果cluster的调度系统在这台机器上又调度了其他的任务，由于CPU、内存、本地磁盘和网络带宽等竞争因素的存在，导致执行MapReduce代码的执行效率更加缓慢。我们最近遇到的一个问题是由于机器的初始化代码有bug，导致关闭了处理器的缓存：在这些机器上执行任务的性能和正常情况相比有上百倍的差距。

我们有一个通用的机制来减少"落伍者"出现的情况。当一个MapReduce操作接近完成的时候，master调度备用(backup)任务进程来执行剩下的、处于处理中状态(in-progress)的任务。无论是最初的执行进程、还是备用(backup)任务进程完成了任务，我们都把这个任务标记成为已经完成。我们调优了这个机制，通常只会占用比正常操作多几个百分点的计算资源。我们发现采用这样的机制对于减少超大MapReduce操作的总处理时间效果显著。比如，5.3描述的排序程序在关闭备用任务的情况下要多花44%的时间才能完成。

## 4 技巧

虽然简单的写一个Map函数和一个Reduce函数提供的基本功能已经能够满足大部分应用的需要，我们还是发现了一些有价值的扩展功能。本节将描述这些扩展功能。

### 4.1 分区函数

MapReduce的使用者通常会指定Reduce任务和Reduce任务输出文件的数量(R)。数据通过在中间key上使用分区函数来分割这些任务。一个缺省的分区函数是使用hash方法(比如，"hash(key) mod R")进行分区。这样的分区方法能产生非常平衡的分区。然而，有的时候，其它的一些分区函数对key值进行的分区将是非常有用的。比如，有时候MapReduce的输出key值是URLs，我们希望每个主机的所有条目保持在同一个输出文件中。为了支持类似的情况，MapReduce库的用户需要提供专门的分区函数。例如，使用"hash(Hostname(urlkey)) mod R"作为分区函数就可以把所有来自同一个主机的URLs保存在同一个输出文件中。

### 4.2 顺序保证

我们确保在给定的分区中，中间key/value pair数据的处理顺序是按照key值增量顺序处理的。这样的顺序保证对每个分区生成一个有序的输出文件，这对于需要对输出文件按key值随机存取的应用非常有意义，对在排序输出的数据集也很有帮助。

### 4.3 Combiner函数

在某些情况下，Map函数产生的中间key值的重复数据会占很大的比重，并且，用户自定义的Reduce函数满足结合律和交换律。2.1节的"单词计数"程序是一个很好的例子。由于词频率遵循一个zipf分布(齐夫分布)，每个Map任务将产生成千上万个这样的记录<the, 1>。所有的这些记录将通过网络被发送到一个单独的Reduce任务，然后由这个Reduce任务把所有这些记录累加起来产生一个数字。我们允许用户指定一个可选的combiner函数，combiner函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出去。

Combiner函数在每台执行Map任务的机器上都会被执行一次。一般情况下，Combiner和Reduce函数是一样的。Combiner函数和Reduce函数之间唯一的区别是MapReduce库怎样控制函数的输出。Reduce函数的输出被保存在最终的输出文件里，而Combiner函数的输出被写到中间文件里，然后被发送给Reduce任务。

部分的合并操作可以显著的提高某些MapReduce操作的速度。附录A包含一个使用combiner函数的例子。

### 4.4 输入和输出的类型

MapReduce库支持几种不同的格式的输入数据。比如，文本模式的输入把每一行作为一个key/value pair：key是文件中行的偏移量，value是行的内容。另外一种常见的格式是以key进行排序来存储的key/value pair的序列。每种输入类型的实现都必须能够把输入数据分割成数据片段，该数据片段能够由单独的Map任务来进行后续处理(例如，文本模式的范围分割必须确保仅仅在行的边界进行范围分割)。虽然大多数MapReduce的使用者仅仅使用很少的几个预定义的输入类型就满足要求了，但是使用者依然可以通过提供一个简单的Reader接口实现就能够支持一个新的输入类型。

Reader并非一定要从文件中读取数据，比如，我们可以很容易的实现一个从数据库里读记录的Reader，或者从内存中的数据结构读取数据的Reader。

类似的，我们提供了一些预定义的输出数据的类型，并且提供了用户扩展新的输出类型的机制。

### 4.5 副作用

在某些情况下，MapReduce的使用者发现，如果在Map和/或Reduce操作过程中增加辅助的输出文件会比较省事。我们依靠程序writer把这种"副作用"变成原子的和幂等的。通常应用程序首先把输出结果写到一个临时文件中，在输出全部数据之后，在使用系统级的原子操作rename重新命名这个文件。

如果一个任务产生了多个输出文件，我们没有提供类似两阶段提交的原子操作支持这种情况。因此，对于会产生多个输出文件、并且对于跨文件有一致性要求的任务，都应该是确定性的。但是在实际应用过程中，这个限制还没有给我们带来过麻烦。

### 4.6 跳过损坏的记录

有时候，用户程序中的bug导致Map或者Reduce函数在处理某些记录的时候crash掉，MapReduce操作无法顺利完成。惯常的做法是修复bug后再次执行MapReduce操作，但是，有时候找出这些bug并修复它们不是一件容易的事情；也许这些bug是在第三方库里面，而我们手头没有这些库的源代码。而且，有时候忽略一些有问题的记录也是可以接受的，比如在一个巨大的数据集上进行统计分析的时候。我们提供了一种执行模式，在这种模式下，为了保证保证整个处理能够顺利进行，MapReduce库将检测哪些记录导致确定性的crash，并且跳过这些记录不处理。

每个worker进程都设置了信号处理函数捕获内存段异常和总线错误。在执行Map或者Reduce操作之前，MapReduce库通过全局变量保存记录序号。如果用户程序触发了一个系统信号，消息处理函数将用"最后一口气"通过UDP包向MapReduce的master发送处理的最后一条记录的序号。当master看到在处理某条特定记录时不止一个worker失效了，master就将标记这条记录需要被跳过，并且在下次重新执行相关的Map或者Reduce任务的时候跳过这条记录。

### 4.7 本地执行

调试Map或者Reduce函数的bug是非常困难的，因为实际执行MapReduce操作时不但是分布在系统中执行的，而且通常是在由几千台计算机组成的系统中执行，具体的执行位置是由master动态的进行调度的，这又大大增加了调试的难度。为了简化调试、测试和小规模测试，我们开发了一套MapReduce库的替代实现，这个实现把所有执行MapReduce操作的处理进程都放在本地机器上顺序的执行。用户可以控制MapReduce操作的执行，可以把操作限制到特定的Map任务上。用户通过设定特别的标志来在本地执行他们的程序，之后就可以很容易的使用本地调试和测试工具(比如gdb)。

### 4.8 状态信息

master使用嵌入式HTTP服务器显示一组状态信息页面，用户可以监控各种执行状态。状态信息页面显示了包括计算执行的进度，比如已经完成了多少任务、有多少任务正在处理、输入的字节数、中间数据的字节数、输出的字节数、处理百分比等等。页面还包含了指向每个任务的stderr和stdout文件的链接。用户根据这些数据预测计算需要执行大约多长时间、是否需要向计算过程中添加更多的资源。这些页面也可以用来分析什么时候计算执行的比预期的要慢。

另外，处于最高级别的状态页面显示了哪些worker失效了，以及他们失效的时候正在运行哪个Map和Reduce任务。这些信息对于调试用户代码中的bug很有帮助。

### 4.9 计数器

MapReduce库使用计数器统计不同事件发生次数。比如，用户可能想统计已经处理了多少个单词、已经索引的多少个德语文档等等。

为了使用这个特性，用户在程序中创建一个命名的计数器对象，在Map和/或Reduce函数中相应的增加计数器的值。例如：

```
Counter* uppercase;
uppercase = GetCounter("uppercase");

map(String name, String contents):
    for each word w in contents:
        if (IsCapitalized(w)):
            uppercase->Increment();
        EmitIntermediate(w, "1");
```

来自单个worker机器的计数器周期性的传递给master(附加在ping的应答包中传递)。master把执行成功的Map和Reduce任务的计数器值进行累计，当MapReduce操作完成之后，返回给用户代码。当前计数器的值也会显示在master的状态页面上，这样用户就可以看到当前计算的进度。当累加计数器的值的时候，master要检查重复运行的Map或者Reduce任务，避免重复累加(之前提到的备用任务和失效后重新执行任务这两种情况会导致相同的任务被多次执行)。

有些计数器的值是由MapReduce库自动维持的，比如已经处理的输入的key/value pair的数量、输出的key/value pair的数量等等。

计数器机制对于MapReduce操作的完整性检查很有用。比如，在某些MapReduce操作中，用户需要确保输出的key/value pair数量和输入的key/value pair数量完全相等，或者处理的德语文档数量在处理的整个文档数量中是否在一个可以接受的范围内。

## 5 性能

本节我们在两个运算上测试MapReduce的性能。一个是在大约1TB的数据中搜索特定的模式，另一个是对大约1TB的数据进行排序。

这两个程序代表了MapReduce用户编写的真实程序的一大类–一类是把数据从一种表现形式转换为另外一种表现形式，另外一类是从海量数据中抽取少部分的用户感兴趣的数据。

### 5.1 集群配置

所有这些程序都运行在一个大约由1800台机器组成的集群上。每台机器配置2个2GHz的Intel Xeon处理器，支持超线程，4GB的内存，两个160GB的IDE硬盘，以及一个千兆的以太网卡。这些机器部署在一个两级的树形交换网络中，在根节点大概有100-200Gbps的总带宽可用。所有这些机器都采用相同的部署(配置)，因此任意两台机器之间的网络来回时间小于1毫秒。

在4GB的内存里，大概有1-1.5GB用于运行在集群上的其他任务。测试程序在周末下午开始执行，这时主机的CPU、磁盘和网络基本上处于空闲状态。

### 5.2 Grep

这个grep程序扫描10^10个100字节的记录，查找一个相对较少出现的3字符的模式(这个模式在92337个记录中出现)。输入数据被拆分成大约64MB的Block(M=15000)，整个输出数据存放在一个文件中(R=1)。

图2显示了这个运算过程随时间的处理过程。其中Y轴表示输入数据的处理速度。处理速度随着参与MapReduce计算的机器数量的增加而增加，当1764台worker参与计算的时候，处理速度达到峰值30GB/s。当Map任务结束的时候，即在计算开始后80秒，输入的处理速度降到0。整个计算过程从开始到结束一共花了大约150秒。这包括了大约一分钟的初始启动时间。初始启动时间包括了把这个程序传播到所有参与计算的worker机器上、等待GFS文件系统打开1000个输入文件的集合以及获取相关的本地化优化信息的时间。

### 5.3 排序

排序程序处理10^10个100字节的记录(大概1TB的数据)。这个程序模仿TeraSort benchmark。

排序程序由不到50行的用户代码组成。只有三行的Map函数从文本行中提取10个字节的排序key，并且把这个key和原始文本行作为中间的key/value pair输出。我们使用了一个内置的恒等函数作为Reduce操作。这个函数把中间的key/value pair不作任何改变输出。最终排序结果输出到一个2路复制的GFS文件系统(也就是说，程序输出2TB的数据)。

如前所述，输入数据被分割成64MB的数据块(M=15000)。我们把排序后的输出结果分区后存储到4000个文件中(R=4000)。分区函数使用key的原始字节来把数据分区到R个片段中。

我们这个benchmark的分区函数知道key的分布情况。在一般的排序程序中，我们会增加一个预处理的MapReduce操作用于采样key的分布，然后利用采样的结果来计算出一个比较好的分区函数，进行真正的排序处理。

图3(a)显示了这个排序程序的执行过程。左上的图显示输入文件读取的速度。数据读取速度峰值大约是13GB/s，并且所有Map任务完成之后迅速滑落到0。值得注意的是，输入数据读取速度比grep例子中的小一些，这是因为排序的Map任务花了大约一半的处理时间和I/O带宽把中间输出结果写到本地硬盘。grep例子相应的中间输出结果的大小是可以忽略的。

左中的图显示了中间数据从Map任务发送到Reduce任务的网络速度。这个数据混排(shuffle)的过程在第一个Map任务完成之后就开始了。图中第一个高峰是第一批大约1700个Reduce任务(整个MapReduce分布到大约1700台机器上，每台机器一次最多执行一个Reduce任务)。计算运行大约300秒后，第一批Reduce任务中的一些执行完成了，我们开始执行剩余的Reduce任务。所有的混排(shuffle)过程在计算开始后大约600秒的时候结束。

左下的图显示Reduce任务把排序后的数据写到最终的输出文件的速度。在第一个混排阶段结束和数据开始写入磁盘之间有一个延时，这是因为机器正忙于排序中间数据。写入过程以2-4GB/s的速度持续一段时间。输出数据写入磁盘大约持续850秒。计入初始启动时间，整个运算消耗了891秒。这个结果和TeraSort benchmark的最好结果1057秒相近。

有几个值得注意的现象：由于我们的本地化优化策略–输入数据读取速度比混排速度和输出数据写入速度要高，这是因为大部分数据都是从本地硬盘读取的，从而节约了网络带宽；混排速度比输出速度高，这是因为输出阶段要写两个副本(我们使用了2路GFS复制存储输出结果)；输入阶段没有混排和输出阶段，是因为输入阶段没有对应的依赖关系–在所有Map任务执行完毕之前，Reduce任务无法执行。

### 5.4 备用任务的效果

在图3(b)中，我们显示了关闭备用任务后排序程序执行情况。执行的过程和图3(a)很相似，除了输出阶段的尾部有一个很长的拖尾，而且在这段时间里，几乎没有什么写入动作。在960秒后，除了5个Reduce任务没有完成外，其他所有的Reduce任务都完成了。但是这最后几个拖后腿的任务又执行了300秒才完成。整个计算耗费了1283秒，多了44%的执行时间。

### 5.5 机器失效

在图3(c)中，我们显示了排序程序执行过程中，我们故意kill了1746个worker中的200个进程。集群底层的调度把这些机器上的进程在30秒内重新启动了(由于只是kill了进程，机器还在工作，所以进程重启是很快的)。

worker的死亡显示为输入处理速度的一个负的跳变，因为一些已经完成的Map任务丢失了(由于相应的Map任务的worker进程被kill了)，需要重新执行。相关Map任务很快就被重新执行了。整个运算在933秒内完成，包括了初始启动时间(只比正常执行时间增加了5%)。

## 6 经验

我们在2003年1月完成了第一个版本的MapReduce库，在2003年8月的版本有了显著的增强，这个增强包括了本地化优化、worker机器之间的动态负载均衡等等。从那以后，我们惊喜的发现，MapReduce库能够广泛应用于我们日常处理的各种问题。它现在在Google内部各个领域都有应用，包括：

- 大规模机器学习问题
- Google News和Froogle产品的集群问题
- 从公众查询产品(比如Google的Zeitgeist)的报告中抽取数据。
- 从大量的新应用和新产品的网页中提取有用信息(比如，从大量的位置搜索网页中抽取地理位置信息)。
- 大规模的图形计算。

图4显示了在我们的源代码管理系统中，随着时间推移，独立的MapReduce程序数量的显著增加。从2003年早期的0个增长到2004年9月份的差不多900个不同的程序。MapReduce的成功取决于采用MapReduce库能够在不到半个小时时间内写出一个简单的程序，这个简单的程序能够在上千台机器的组成的集群上做大规模并发处理，这极大的加快了开发和原形设计的周期。另外，采用MapReduce库，让完全没有分布式和/或并行系统开发经验的程序员也能很容易的利用大量的资源开发分布式应用。

在每个任务结束的时候，MapReduce库统计计算资源的使用状况。在表1，我们列出了2004年8月份MapReduce运行的一些任务的统计。

### 6.1 大规模索引

迄今为止，我们对MapReduce最重要的一个应用就是完全重写了索引生成系统，其生成的数据结构被用于Google web的搜索服务。索引系统将我们的爬虫系统所检索到的、被存储为一系列GFS文件的大量文档作为输入。这些文档的原始内容的数据大小超过了20TB。整个索引处理过程由5到10个连续的MapReduce操作组成。使用MapReduce(而不是之前版本索引系统的点对点分布式传输)能带来几个好处:

索引相关的代码变得更简单、(代码量)更少和更容易理解，因为处理容错、分布式和并行化的代码被隐藏在了MapReduce库内部。例如，某一计算阶段的代码量在(改为)使用MapReduce表达后从(原来的)大约3800行c++代码降低至大约700行。

MapReduce库的性能是足够好的，这使得我们可以将概念上无关的计算进行拆分，而不是将它们混合在一起，从而避免额外的数据传输。这使得可以简单的改变索引的处理过程。举个例子，在我们老的索引系统中曾进行的一次改动耗费了我们几个月的时间，而在新系统中去实现则只需要几天时间。

处理索引变得更容易操作，因为大多数机器故障、机器执行缓慢和网络间歇性断开(networking hiccups)等问题都由MapReduce库自动处理了，而不需操作员介入。此外，通过向索引处理的集群中添加新的机器可以轻松地提高索引处理的性能。

## 7 相关工作

很多系统都提供了受限制的编程模型，并且使用这些约束来自动的将计算并行化。举个例子，使用并行前缀计算时，一个结合函数可以在N个处理器上，以logN的时间计算出一个N元素数组的所有前缀。MapReduce可以被认为是基于我们在现实世界中关于大型计算的经验所总结出的一些模型的一个简化和精炼。更重要的是，我们提供了一个可拓展到几千个处理器规模的容错实现。相比之下，大多数的并行处理系统的实现只能运用在更小的规模下，并且将处理机器故障的细节留给了程序员(去实现)。

整体同步程序(Bulk Synchronous Programming)和一些消息传递接口(MPI Message-Passing Interface)原语提供了更高级别的抽象，使得程序员可以更加简单的编写并行程序。这些系统与MapReduce最关键的不同在于MapReduce利用一个受限的编程模型令用户程序自动的并行化并且了提供透明的(用户无需感知的)容错机制。

我们局部性优化机制的灵感源自active disks等技术，推进计算并使得所要处理的元素是靠近本地磁盘的，以减少通过网络I/O子系统发送的数据量。我们的计算运行在直连少量磁盘的商用处理器上，而不是直接运行在有着磁盘控制器的处理器(disk controller processors)上，但大致的方法是类似的。

我们的后备任务机制类似于Charlotte系统中所应用的紧急调度(eager scheduling)机制。简单的紧急调度机制的一个缺点就是如果一个给定的任务反复失败，则整个计算将无法完成。我们通过跳过有问题记录的机制，一定程度上的修复了这一问题。

MapReduce的实现依赖于一个内部的集群管理系统，该系统负责在大量的共享机器中分发和运行用户的任务。虽然这并不是本论文的重点，但该集群管理系统从本质上来说和Condor系统非常相似。

排序机制做为MapReduce库的一部分，在操作上与NOW-Sort类似。源机器(map workers)将待排序的数据进行分区，并将其发送给R个reduce worker中的一个。每一个reduce worker在本地对数据进行(尽可能的在内存中排序)。当然，NOW-Sort不支持使得可用户自定义的Map和Reduce函数，相比之下我们的MapReduce库则有着更广的适用范围。

River提供了一个编程模型，该模型中进程间通过向分布式队列中发送数据来进行通信。和MapReduce一样，即使由于异构的硬件或者系统扰动而导致了(计算资源的)不均衡，River系统也试图在这种场景下提供足够均衡的性能。River通过仔细的对磁盘和网络传输进行调度，用以实现任务完成时间的平衡。MapReduce则采用了不同的方法。通过受限的编程模型，MapReduce框架能够将一个问题分割为大量细粒度的任务。这些任务会在可用的worker机器上动态的调度，因此运行速度更快的worker能够处理更多的任务。这一受限的编程模型也允许我们在job接近完成时进行冗余任务的调度，这可以极大地减少在非均衡场景下的任务完成时间(比如存在缓慢或者卡住不动的worker)。

BAD-FS是一个与MapReduce非常不同的编程模型。与MapReduce不同，其致力于跨广域网的执行job。然而，这里有两个很相似的基本点。两个系统都使用冗余的执行来恢复由故障导致的数据丢失。两者都使用距离敏感的调度策略，用以减少在拥挤的网络链路上所发送数据的数量。

TACC是一个旨在简化高性能网络服务构造的框架。和MapReduce一样，其也依赖重复执行机制来实现故障容错。

## 8 总结

MapReduce编程模型已经成功的在谷歌中被广泛应用。我们认为这一成功出于几个原因。首先，这一模型很容易使用，因为其隐藏了并行化、故障容错、局部性优化以及负载均衡的细节，即使是没有并行计算和分布式系统经验的程序员也能轻松地使用。其次，各种各样的问题都能用MapReduce计算轻松地表达。例如，MapReduce被用于为谷歌的网络搜索产品生成数据、也被用于排序、用于数据挖掘、用于机器学习以及其它的很多系统。再次，我们已开发的MapReduce实现可以被扩展到包含数千台机器的大型集群中。这一实现使得众多机器资源能被有效的利用，因此其很适合谷歌所遇到的许多大型计算问题。

我们从这项工作中学到了一些事情。首先，受限制的计算模型能够简化并行化和分布式计算，并且能够令这些计算具有容错性。其次，网络带宽是一种稀缺资源。因此我们的系统中有许多致力于减少在网络中传输数据数量的优化：局部性优化允许我们从本地磁盘中读取数据，以及将中间态数据的单个备份写入本地磁盘以节约网络带宽。再次，冗余的重复执行可以用于减少慢机器的影响，以及处理机器故障和数据丢失。

## 致谢

Josh Levenberg基于他使用MapReduce的经验以及其它人提出的优化建议，在修改MapReduce的用户级API和为其拓展很多新特性的过程中发挥了重要作用。MapReduce是基于谷歌文件系统GFS读取输入数据和写出输出数据的。我们要感谢Mohit Aron、Howard Gobioff、Markus Gutschke、David Kramer、Shun Tak Leung和Josh Redstone为开发GFS所做的工作。我们也要感谢Percy Liang和Olcan Sercinoglu为开发MapReduce集群管理系统所做的工作。Mike Burrows, Wilson Hsieh, Josh Levenberg, Sharon Perl, Rob Pike和Debby Wallach为这篇论文的前期草稿提供了很有帮助的建议。匿名的OSDI评论员和我们的审核者Eric Brewer就论文可以改进的方面提供了许多有用的建议。最后，我们感谢谷歌工程部的所有MapReduce用户，感谢他们提供的有价值的反馈、建议和bug报告。

---

**译者注**: 本文翻译自Google发表的MapReduce经典论文，翻译过程中参考了多个版本的中文翻译，力求准确传达原文含义。MapReduce作为大数据处理的奠基性技术，对后续的Hadoop、Spark等分布式计算框架产生了深远影响。