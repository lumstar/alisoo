---
title: "The Google File System: 可扩展的分布式文件系统"
date: "2024-12-16"
category: "大数据"
tags: ["GFS", "Google", "分布式存储", "大数据", "分布式系统", "文件系统"]
excerpt: "Google File System (GFS) 是Google设计并实现的可扩展分布式文件系统，为大规模分布式数据密集型应用程序而设计。本文是GFS经典论文的中文翻译版本。"
author: "Google Research"
translator: "叉鸽"
---

# The Google File System: 可扩展的分布式文件系统

**原文标题**: The Google File System  
**作者**: Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung  
**发表**: SOSP 2003  
**原文链接**: https://research.google.com/archive/gfs-sosp2003.pdf

## 摘要

我们设计并实现了Google GFS文件系统，一个面向大规模数据密集型应用的、可伸缩的分布式文件系统。GFS虽然运行在廉价的普遍硬件设备上，但是它依然可以提供容灾功能，并为大量客户端提供高总体性能服务。

虽然GFS的设计目标与以前的分布式文件系统有很多相同之处，但我们的设计却是由对我们的应用程序负载和技术环境的观察所驱动的，这些观察结果既反映了与早期分布式文件系统的假设不同的地方，也反映了与早期分布式文件系统假设相同的地方。这促使我们重新审视了传统的选择，并探索了全新的设计要点。

GFS与传统文件系统的接口不同，但是实现了传统文件系统的基本操作，如创建、删除、打开、关闭、读和写文件。此外，GFS还具有快照和记录追加操作，快照以低成本创建一个文件或者目录树的拷贝，记录追加操作允许多个客户端同时追加数据到同一个文件，同时保证每个客户端追加操作的原子性。

我们的文件系统成功的满足了我们的存储需要。GFS目前作为存储平台广泛的部署在Google内部，为我们的服务产生和处理数据提供存储支持，同时作为需要大数据集的研究和开发工作的存储平台。到目前为止，我们最大的集群通过以太网连接1000多台机器上的3000多个磁盘，提供数百TB的存储空间，同时被数百个客户端持续访问。

在本论文中，我们展示了设计用来支持分布式应用的文件系统接口的扩展，讨论我们设计的许多方面，最后对小规模基准测试和真实使用作了测量报告。

## 1 引言

我们设计并实现了Google文件系统(GFS)，一个面向大规模数据密集型应用的可扩展分布式文件系统。GFS与传统的分布式文件系统有着相同的目标：性能、可扩展性、可靠性和可用性。但是，我们的设计是由对我们的应用程序工作负载和技术环境的观察所驱动的，这些观察结果既反映了与早期分布式文件系统假设的不同，也反映了相同之处。这促使我们重新审视传统的选择，并探索全新的设计要点。

首先，组件失效被认为是常态事件，而不是意外事件。文件系统由数百甚至数千台由廉价的普通部件组装的存储机器组成，同时被相当数量的客户端访问。组件的数量和质量实际上保证了在任何给定时间，一些组件是不工作的，并且有一些组件无法从当前的故障中恢复。我们遇到过由应用程序bug、操作系统bug、人为错误，以及磁盘、内存、连接器、网络和电源故障引起的问题。因此，持续的监控、错误检测、容错和自动恢复必须集成到文件系统中。

第二，以传统标准衡量，文件非常大。数GB的文件是很常见的。每个文件通常包含许多应用程序对象，如Web文档。当我们经常处理快速增长的、包含数十亿个对象、数TB大小的数据集时，管理数十亿个大约KB大小的文件是非常笨拙的，即使文件系统能够支持这一点。因此，设计假设和参数，如I/O操作和块大小，必须重新考虑。

第三，大多数文件的修改是通过追加新数据，而不是覆盖现有数据。文件中的随机写入实际上是不存在的。一旦写入，文件就只能读取，而且通常只能顺序读取。各种各样的数据都有这种特性。有些可能构成大型存储库，数据分析程序会扫描这些存储库。有些可能是正在运行的应用程序连续生成的数据流。有些可能是存档数据。有些可能是在一台机器上产生、在另一台机器上同时或稍后处理的中间结果。鉴于对大文件的这种访问模式，追加成为性能优化和原子性保证的焦点，而客户端中的数据缓存则失去了吸引力。

第四，应用程序和文件系统API的协同设计提高了整个系统的灵活性。例如，我们放宽了GFS的一致性模型，以大大简化文件系统，而不会给应用程序带来繁重的负担。我们还引入了一个原子追加操作，这样多个客户端就可以同时追加到一个文件中，而不需要它们之间的额外同步。这些将在本论文的后面讨论。

多个GFS集群目前已经部署用于不同的目的。最大的集群有超过1000个存储节点，超过300TB的磁盘存储，并且被不同机器上的数百个客户端持续大量访问。

## 2 设计概述

### 2.1 设计假设

在设计文件系统时，我们受到了关键观察结果的指导，这些观察结果反映了我们如何使用工作负载以及我们希望部署系统的技术环境。我们之前提到了一些关键点，现在更详细地列出我们的假设。

- 系统由许多经常失效的廉价的普通组件构成。它必须持续监控自身，并在常规基础上检测、容忍和迅速恢复组件故障。

- 系统存储适度数量的大文件。我们期望有几百万个文件，每个文件通常为100MB或更大。数GB的文件是常见情况，应该被有效管理。小文件必须被支持，但我们不需要为此进行优化。

- 工作负载主要由两种读取组成：大型流式读取和小型随机读取。在大型流式读取中，单个操作通常读取数百KB，更常见的是1MB或更多。来自同一客户端的连续操作通常读取文件的连续区域。小型随机读取通常在文件中的某个任意偏移处读取几KB。注重性能的应用程序通常对其小型读取进行批处理和排序，以便稳步地通过文件前进，而不是来回移动。

- 工作负载还有许多大型的、顺序的写入，将数据追加到文件中。典型的操作大小与读取类似。一旦写入，文件很少再次修改。支持在文件中任意位置的小型写入，但不一定要高效。

- 系统必须为多个客户端并发追加到同一文件的情况实现定义良好的语义。我们的文件经常被用作生产者-消费者队列或用于多路合并。数百个生产者，运行在不同的机器上，将并发地追加到一个文件。原子性以最小的同步开销是必需的。文件可能稍后被读取，或者消费者可能通过文件同时读取。

- 高持续带宽比低延迟更重要。我们的大多数目标应用程序都重视以高速率批量处理数据，很少有对单个读取或写入有严格响应时间要求的应用程序。

### 2.2 接口

GFS提供了一个熟悉的文件系统接口，尽管它没有实现标准的API，如POSIX。文件按目录分层组织，并由路径名标识。我们支持创建、删除、打开、关闭、读取和写入文件的常用操作。

此外，GFS有快照和记录追加操作。快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时将数据追加到同一文件，同时保证每个单独客户端追加的原子性。它对于实现多路合并结果和生产者-消费者队列非常有用，许多客户端可以同时写入而无需额外的锁定。我们发现这些类型的文件对于构建大型分布式应用程序非常有价值。

### 2.3 架构

GFS集群由一个master和多个chunkserver组成，并被多个客户端访问，如图1所示。每个都是一个普通的Linux机器，运行用户级服务器进程。在同一台机器上同时运行chunkserver和客户端是很容易的，只要机器资源允许，我们的工作负载质量可以接受。

文件被分成固定大小的chunk。每个chunk由一个不可变的、全局唯一的64位chunk handle标识，该handle在chunk创建时由master分配。Chunkserver将chunk作为Linux文件存储在本地磁盘上，并通过指定chunk handle和字节范围来读写chunk数据。为了可靠性，每个chunk被复制到多个chunkserver上。默认情况下，我们存储三个副本，尽管用户可以为文件命名空间的不同区域指定不同的复制级别。

master维护所有文件系统元数据。这包括命名空间、访问控制信息、从文件到chunk的映射，以及chunk的当前位置。它还控制系统范围的活动，如chunk租约管理、孤立chunk的垃圾收集，以及chunkserver之间的chunk迁移。master通过HeartBeat消息定期与每个chunkserver通信，给它指令并收集其状态。

链接到每个应用程序的GFS客户端代码实现文件系统API，并与master和chunkserver通信，代表应用程序读写数据。客户端与master交互进行元数据操作，但所有承载数据的通信都直接发送到chunkserver。我们不提供POSIX API，因此不需要挂接到Linux vnode层。

客户端和chunkserver都不缓存文件数据。客户端缓存提供很少的好处，因为大多数应用程序流过大文件或工作集太大而无法缓存。不缓存简化了客户端和整个系统，消除了缓存一致性问题。(但是，客户端确实缓存元数据。)Chunkserver不需要缓存文件数据，因为chunk作为本地文件存储，Linux的缓冲区缓存已经将经常访问的数据保存在内存中。

### 2.4 单一Master

拥有单一master大大简化了我们的设计，并使master能够使用全局知识做出复杂的chunk放置和复制决策。但是，我们必须最小化其在读写中的参与，以免它成为瓶颈。客户端从不通过master读写文件数据。相反，客户端询问master它应该联系哪些chunkserver。它将此信息缓存一段有限的时间，并直接与chunkserver交互进行许多后续操作。

让我们用图1解释简单读取的交互。首先，使用固定的chunk大小，客户端将应用程序指定的文件名和字节偏移转换为文件内的chunk索引。然后，它向master发送包含文件名和chunk索引的请求。master回复相应的chunk handle和副本的位置。客户端使用文件名和chunk索引作为键缓存此信息。

然后客户端向其中一个副本发送请求，最可能是最近的副本。请求指定chunk handle和该chunk内的字节范围。对同一chunk的进一步读取不需要更多的客户端-master交互，直到缓存的信息过期或文件被重新打开。实际上，客户端通常在同一请求中询问多个chunk，master也可以立即包含那些请求的chunk之后的chunk的信息。这个额外的信息在几乎没有额外成本的情况下避免了几个未来的客户端-master交互。

### 2.5 Chunk大小

Chunk大小是关键设计参数之一。我们选择了64MB，这比典型的文件系统块大小大得多。每个chunk副本作为普通Linux文件存储在chunkserver上，并且只在需要时扩展。惰性空间分配避免了由于内部碎片而浪费空间，这可能是对这种大chunk大小的最大反对意见。

大chunk大小提供了几个重要优势。首先，它减少了客户端与master的交互需求，因为对同一chunk的读写只需要一个对master的初始请求来获取chunk位置信息。这种减少对我们的工作负载特别重要，因为应用程序大多顺序读写大文件。即使对于小的随机读取，客户端也可以轻松缓存几TB工作集的所有chunk位置信息。其次，由于在大chunk上，客户端更可能对给定chunk执行许多操作，它可以通过与chunkserver保持持久的TCP连接来减少网络开销。第三，它减少了存储在master上的元数据的大小。这允许我们将元数据保存在内存中，这带来了我们将在2.6.1节中讨论的其他优势。

另一方面，大chunk大小，即使使用惰性空间分配，也有缺点。小文件由少数chunk组成，也许只有一个。如果许多客户端访问同一小文件，存储这些chunk的chunkserver可能会成为热点。在实践中，热点还没有成为主要问题，因为我们的应用程序大多顺序读取包含许多chunk的大文件。

但是，当GFS首次被用于批处理队列系统时，热点确实出现了：一个可执行文件作为单chunk文件写入GFS，然后同时在数百台机器上启动。存储此可执行文件的少数chunkserver被数百个同时请求过载。我们通过以更高的复制因子存储此类可执行文件，并使批处理队列系统错开应用程序启动时间来解决此问题。长期解决方案是允许客户端在这种情况下从其他客户端读取数据。

### 2.6 元数据

master存储三种主要类型的元数据：文件和chunk命名空间、从文件到chunk的映射，以及每个chunk副本的位置。所有元数据都保存在master的内存中。前两种类型(命名空间和文件到chunk映射)也通过将变更记录到存储在master本地磁盘上并在远程机器上复制的操作日志中来持久保存。使用日志允许我们简单可靠地更新master状态，并且在master崩溃时不会有不一致的风险。master在启动时不会持久存储chunk位置信息。相反，它在启动时询问每个chunkserver其chunk，并且每当chunkserver加入集群时。

#### 2.6.1 内存中数据结构

由于元数据存储在内存中，master操作很快。此外，master可以简单高效地在后台定期扫描其整个状态。这种定期扫描用于实现chunk垃圾收集、在chunkserver故障时重新复制，以及chunk迁移以平衡chunkserver之间的负载和磁盘空间使用。第4.3和4.4节将讨论这些活动。

这种内存方法的一个潜在关注点是chunk数量，因此整个系统的容量受到master有多少内存的限制。在实践中，这不是一个严重的限制。master为每个64MB chunk维护少于64字节的元数据。大多数chunk都是满的，因为大多数文件包含许多chunk，只有最后一个可能部分填充。类似地，文件命名空间数据通常每个文件需要少于64字节，因为它使用前缀压缩紧凑地存储文件名。

如果需要支持更大的文件系统，为master添加额外内存的成本相对较小，与增加整个系统存储容量的好处相比。

#### 2.6.2 Chunk位置

master不保持chunk副本位置的持久记录。相反，它在启动时询问每个chunkserver它有哪些chunk，并且每当chunkserver加入集群时。master可以保持这些信息是最新的，因为它控制所有chunk放置，并通过定期HeartBeat消息监控chunkserver状态。

我们最初试图在master上持久保存chunk位置信息，但我们决定在启动时从chunkserver请求数据更简单。这消除了在chunkserver加入和离开集群、更改名称、失败、重新启动等时保持master和chunkserver同步的问题。在有数百个服务器的集群中，这些事件经常发生。

理解这种设计决策的另一种方式是认识到chunkserver对其磁盘上的chunk拥有最终发言权。在master上维护这些信息的一致视图没有意义，因为chunkserver上的错误可能导致chunk自发消失(例如，磁盘可能损坏并被禁用)，或者操作员可能重命名chunkserver。

#### 2.6.3 操作日志

操作日志包含关键元数据更改的历史记录。它对GFS至关重要。它不仅是元数据的唯一持久记录，而且还用作定义并发操作顺序的逻辑时间线。文件和chunk，以及它们的版本，都由它们创建的逻辑时间唯一且永久标识。

由于操作日志至关重要，我们必须可靠地存储它，并且在元数据更改持久化之前不使更改对客户端可见。否则，即使chunk本身存活，我们实际上也会丢失整个文件系统或最近的客户端操作。因此，我们在多个远程机器上复制它，并且只有在将相应的日志记录刷新到本地和远程磁盘后才响应客户端操作。master在刷新几个日志记录之前批处理它们，从而减少刷新和复制对整体系统吞吐量的影响。

master通过重放操作日志来恢复其文件系统状态。为了最小化启动时间，我们必须保持日志小。每当日志增长超过某个大小时，master就会检查其状态，以便它可以通过从本地磁盘加载最新检查点并且只重放有限数量的日志记录来恢复。检查点采用紧凑的B树形式，可以直接映射到内存中并用于命名空间查找，而无需额外的解析。这进一步加快了恢复速度并提高了可用性。

因为构建检查点可能需要一段时间，master的内部状态被构造为使得新检查点可以在不延迟传入变更的情况下创建。master切换到新的日志文件并在单独的线程中创建新检查点。新检查点包括切换之前的所有变更。对于有数百万文件的集群，它可以在一分钟左右完成。完成后，它被写入本地和远程磁盘。

恢复只需要最新的完整检查点和后续的日志文件。较旧的检查点和日志文件可以被删除，尽管我们保留一些以防灾难。检查点期间的故障不会影响正确性，因为恢复代码检测并跳过不完整的检查点。

## 3 系统交互

我们设计系统以最小化master在所有操作中的参与。在这种背景下，我们现在描述客户端、master和chunkserver如何交互以实现数据变更、原子记录追加和快照。

### 3.1 租约和变更顺序

变更是改变chunk内容或元数据的操作，如写入或追加操作。每个变更都在chunk的所有副本上执行。我们使用租约来维护副本之间的一致变更顺序。master将chunk租约授予其中一个副本，我们称之为主副本。主副本为chunk的所有变更选择串行顺序。所有副本在应用变更时都遵循此顺序。因此，全局变更顺序首先由master选择的租约授予顺序定义，然后在租约内由主副本分配的序列号定义。

租约机制旨在最小化master的管理开销。租约的初始超时为60秒。但是，只要chunk正在被变更，主副本就可以请求并通常从master接收扩展以获得无限期的扩展。这些扩展请求和授予被附加到master和所有chunkserver之间定期交换的HeartBeat消息中。master有时可能会尝试在租约到期之前撤销租约(例如，当master想要禁用正在重命名的文件上的变更时)。即使master失去与主副本的通信，它也可以在旧租约到期后安全地将租约授予新副本。

在图2中，我们通过遵循写入的控制流来说明此过程。

1. 客户端询问master哪个chunkserver持有chunk的当前租约以及其他副本的位置。如果没有人有租约，master将租约授予它选择的副本(未显示)。

2. master回复主副本的身份和其他(次要)副本的位置。客户端缓存此数据以供将来变更。只有当主副本无法访问或回复它不再持有租约时，它才需要再次联系master。

3. 客户端将数据推送到所有副本。客户端可以以任何顺序执行此操作。每个chunkserver将数据存储在内部LRU缓冲区缓存中，直到数据被使用或老化。通过将数据流与控制流分离，我们可以通过基于网络拓扑调度昂贵的数据流来提高性能，而不管哪个chunkserver是主副本。第3.2节进一步讨论了这一点。

4. 一旦所有副本都确认接收到数据，客户端就向主副本发送写请求。请求标识之前推送到所有副本的数据。主副本为它接收的所有变更分配连续的序列号，可能来自多个客户端，这提供了必要的序列化。它按序列号顺序将变更应用于其本地状态。

5. 主副本将写请求转发给所有次要副本。每个次要副本按照主副本分配的相同序列号顺序应用变更。

6. 次要副本都回复主副本，指示它们已完成操作。

7. 主副本回复客户端。在任何副本遇到的任何错误都会报告给客户端。在出现错误的情况下，写入可能在主副本和次要副本的任意子集上成功。(如果它在主副本上失败，它就不会被分配序列号并转发。)客户端请求被认为失败，修改的区域处于不一致状态。我们的客户端代码通过重试失败的变更来处理此类错误，在回退到从头开始重试之前，它将首先尝试步骤3到7几次。

如果应用程序的写入很大或跨越chunk边界，GFS客户端代码将其分解为多个写操作。它们都遵循上述控制流，但可能与来自其他客户端的并发操作交错并被覆盖。因此，共享文件区域可能最终包含来自不同客户端的片段，尽管副本将是相同的，因为各个操作在所有副本上以相同顺序完成。这使文件区域处于一致但未定义的状态，如第2.7节所述。

### 3.2 数据流

我们将数据流与控制流分离以有效地使用网络。虽然控制流从客户端到主副本再到所有次要副本，但数据沿着精心选择的chunkserver链线性推送。我们的目标是充分利用每台机器的网络带宽，避免网络瓶颈和高延迟链接，并最小化推送所有数据的延迟。

为了充分利用每台机器的网络带宽，数据沿着chunkserver链线性推送，而不是分布在其他拓扑中(例如树)。因此，每台机器的全部出站带宽用于尽可能快地传输数据，而不是在多个接收者之间分配。

为了尽可能避免网络瓶颈和高延迟链接(例如，交换机间链接通常是两者)，每台机器将数据转发到网络拓扑中尚未接收到数据的"最近"机器。假设客户端将数据推送到chunkserver S1。然后S1将其转发到最接近S1的chunkserver S2。S2将其转发到最接近S2的chunkserver S3，该chunkserver距离S1或S2最远，依此类推。我们的网络拓扑足够简单，可以通过IP地址准确估计"距离"。

最后，我们通过通过TCP连接流水线数据传输来最小化延迟。一旦chunkserver接收到一些数据，它立即开始转发。流水线对我们特别有帮助，因为我们使用全双工以太网链接的交换网络。立即发送数据不会降低接收速率。

### 3.3 原子记录追加

GFS提供原子追加操作，称为记录追加。在传统写入中，客户端指定要写入数据的偏移量。对同一区域的并发写入不是可序列化的：区域可能最终包含来自多个客户端的数据片段。在记录追加中，但是，客户端只指定数据。GFS将其原子地追加到文件中，至少一次，在GFS选择的偏移量处，并将该偏移量返回给客户端。这类似于在Unix中以O_APPEND模式打开的文件写入，多个写入者同时写入而没有竞争条件。

记录追加在我们的分布式应用程序中大量使用，其中来自不同机器的许多客户端同时追加到同一文件。如果使用传统写入，客户端将需要额外的复杂且昂贵的同步，例如通过分布式锁管理器。在我们的工作负载中，此类文件通常用作多个生产者/单个消费者队列或包含来自许多不同客户端的合并结果。

记录追加是一种变更，遵循第3.1节中的控制流，只是在主副本上有一些额外的逻辑。客户端将数据推送到文件最后一个chunk的所有副本，然后将其请求发送到主副本。主副本检查将记录追加到当前chunk是否会导致chunk超过最大大小(64MB)。如果是这样，它将chunk填充到最大大小，告诉次要副本执行相同操作，并回复客户端，指示应在下一个chunk上重试操作。(记录追加被限制为最大chunk大小的四分之一，以保持最坏情况的碎片在可接受的水平。)如果记录适合当前chunk，这是常见情况，主副本将数据追加到其副本，告诉次要副本将数据写入相同偏移量，最后回复客户端成功。

如果记录追加在任何副本上失败，客户端重试操作。结果，同一chunk的副本可能包含不同的数据，可能包括相同记录的重复，全部或部分。GFS不保证所有副本在字节上相同。它只保证数据作为原子单元至少写入一次。这个属性很容易从简单观察中得出，即操作要成功，数据必须在某个chunk的所有副本的相同偏移量处写入。此外，在此之后，所有副本至少与记录结束一样长，因此任何未来记录都将分配更高的偏移量或不同的chunk，即使不同的副本成为主副本。就我们的一致性保证而言，成功记录追加操作写入数据的区域被定义(因此一致)，而中间区域不一致(因此未定义)。我们的应用程序可以处理不一致的区域，如第2.7.2节所述。

### 3.4 快照

快照操作几乎立即制作文件或目录树的副本，同时最小化对正在进行的变更的任何中断。我们的用户使用它来快速创建巨大数据集的分支副本(并且经常递归地创建副本的副本)，或者在实验更改之前检查当前状态，以便稍后可以轻松提交或回滚。

像AFS一样，我们使用标准的写时复制技术来实现快照。当master接收到快照请求时，它首先撤销要快照的文件中chunk的任何未完成租约。这确保对这些chunk的任何后续写入都需要与master交互以找到租约持有者。这将给master一个机会首先创建chunk的新副本。

在租约被撤销或过期后，master将操作记录到磁盘。然后，它通过复制源文件或目录树的元数据将此日志记录应用于其内存状态。新创建的快照文件指向与源文件相同的chunk。

快照操作后客户端第一次想要写入chunk C时，它向master发送请求以找到当前租约持有者。master注意到chunk C的引用计数大于一。它推迟回复客户端请求，而是选择新的chunk句柄C'。然后，它要求每个具有chunk C当前副本的chunkserver创建名为C'的新chunk。通过在与原始chunk相同的chunkserver上创建新chunk，我们确保数据可以在本地复制，而不是通过网络(我们的磁盘大约比我们的100Mb以太网链接快三倍)。从这一点开始，请求处理与任何chunk没有区别：master将租约授予新chunk C'的其中一个副本，并回复客户端，然后客户端可以正常写入chunk，不知道它刚刚从快照创建。

## 4 Master操作

master执行所有命名空间操作。此外，它管理整个系统中的chunk副本：它做出放置决策，创建新chunk及其副本，并协调各种系统范围的活动以保持chunk完全复制，平衡所有chunkserver的负载，并回收未使用的存储。我们现在讨论这些主题中的每一个。

### 4.1 命名空间管理和锁定

许多master操作可能需要很长时间：例如，快照操作必须撤销快照覆盖的所有chunk上的chunkserver租约。我们不希望在它们运行时延迟其他master操作。因此，我们允许多个操作处于活动状态，并使用命名空间区域上的锁来确保正确的序列化。

与许多传统文件系统不同，GFS没有每个目录的数据结构，该结构列出该目录中的所有文件。它也不支持文件或目录的别名(即，Unix术语中的硬链接或符号链接)。GFS在逻辑上将其命名空间表示为将完整路径名映射到元数据的查找表。使用前缀压缩，此表可以在内存中有效表示。命名空间树中的每个节点(绝对文件名或绝对目录名)都有关联的读写锁。

每个master操作在运行之前都会获取一组锁。通常，如果它涉及/d1/d2/.../dn/leaf，它将获取目录名/d1，/d1/d2，...，/d1/d2/.../dn上的读锁，以及完整路径名/d1/d2/.../dn/leaf上的读锁或写锁。请注意，leaf可能是文件或目录，具体取决于操作。

我们现在将说明此锁定方案如何防止在/home/user被快照到/save/user时创建文件/home/user/foo的竞争条件。快照操作获取/home和/save上的读锁，以及/home/user和/save/user上的写锁。文件创建获取/home和/home/user上的读锁，以及/home/user/foo上的写锁。两个操作将被正确序列化，因为它们尝试获取/home/user上的冲突锁。文件创建不需要父目录上的写锁，因为没有"目录"或类似inode的数据结构要修改。读锁足以防止父目录被删除。

这种锁定方案的一个好特性是它允许在同一目录中并发变更。例如，可以在同一目录中同时创建多个文件：每个都获取目录名上的读锁和文件名上的写锁。目录名上的读锁足以防止目录被删除、重命名或快照。文件名上的写锁序列化尝试使用相同名称创建文件两次。

由于命名空间可以有许多节点，读写锁对象是惰性分配的，并且一旦不再使用就被删除。此外，锁以一致的总顺序获取，以防止死锁：它们首先按命名空间树中的级别排序，然后在同一级别内按字典顺序排序。

### 4.2 副本放置

GFS集群在多个级别上高度分布。它通常有数百个chunkserver分布在许多机器机架上。这些chunkserver可能依次被来自相同或不同机架上的数百个客户端访问。不同机架上的两台机器之间的通信可能跨越一个或多个网络交换机。此外，进出机架的带宽可能小于机架内所有机器的总带宽。多级分布为数据分布提出了独特的挑战，以实现可扩展性、可靠性和可用性。

chunk副本放置策略有两个目的：最大化数据可靠性和可用性，以及最大化网络带宽利用率。对于这两个目的，仅在机器之间传播副本是不够的，这只能防止磁盘或机器故障，并充分利用每台机器的网络带宽。我们还必须在机架之间分布chunk副本。这确保即使整个机架损坏或离线(例如，由于共享资源(如网络交换机或电源电路)的故障)，chunk的某些副本仍然存在并保持可用。这也意味着可以利用多个机架的总带宽进行chunk的流量，特别是读取。

另一方面，写入流量必须流过多个机架，这是我们愿意做出的权衡。

### 4.3 创建、重新复制、重新平衡

chunk副本因三个原因而创建：chunk创建、重新复制和重新平衡。

当master创建chunk时，它选择在哪里放置最初的空副本。它考虑几个因素。(1)我们希望将新副本放置在磁盘空间利用率低于平均水平的chunkserver上。(2)我们希望限制每个chunkserver上"最近"创建的数量。虽然创建本身很便宜，但它可靠地预测即将到来的大量写入流量，因为chunk是在需要时创建的，并且在我们的追加一次、读取多次工作负载中，它们通常在创建后立即变得写入繁重。(3)如上所述，我们希望在机架之间传播副本。

当可用副本数量低于用户指定的目标时，master重新复制chunk。这可能由于各种原因而发生：chunkserver变得不可用，它报告其副本可能已损坏，其磁盘之一由于错误而被禁用，或者复制目标增加。每个需要重新复制的chunk根据几个因素进行优先级排序。一个是它距离其复制目标有多远。例如，我们优先重新复制丢失了两个副本的chunk，而不是只丢失了一个副本的chunk。此外，我们更喜欢重新复制活动文件的chunk，而不是最近删除的文件的chunk(参见第4.4节)。最后，为了最小化故障对正在运行的应用程序的影响，我们提高了阻塞客户端进度的任何chunk的优先级。

master选择最高优先级的chunk并通过指示某个chunkserver直接从现有有效副本克隆它来"克隆"它。新副本的放置目标与创建时类似：均衡磁盘空间利用率，限制任何单个chunkserver上的活动克隆操作，并在机架之间传播副本。为了防止克隆流量压倒客户端流量，master限制集群和每个chunkserver上活动克隆操作的数量。此外，每个chunkserver通过限制其对源chunkserver的读取请求来限制其用于每个克隆操作的带宽。

最后，master定期重新平衡副本：它检查当前副本分布并移动副本以获得更好的磁盘空间和负载分布。此外，通过此过程，master逐渐填充新的chunkserver，而不是立即用新chunk和来自其他chunkserver的大量读取流量压倒它。新副本的放置标准与上面讨论的类似。此外，master必须选择要删除的现有副本。一般来说，它更喜欢删除那些在可用空间低于平均水平的chunkserver上的副本，以均衡磁盘空间使用。

### 4.4 垃圾收集

在文件被删除后，GFS不会立即回收可用的物理存储。它只在文件和chunk级别的常规垃圾收集期间惰性地这样做。我们发现这种方法使系统更简单、更可靠。

#### 4.4.1 机制

当应用程序删除文件时，master立即将删除记录到其日志中，就像其他更改一样。但是，文件不是立即回收，而是重命名为包含删除时间戳的隐藏名称。在master对文件系统命名空间的常规扫描期间，它删除任何超过三天的此类隐藏文件(间隔是可配置的)。在那之前，文件仍然可以在新的特殊名称下读取，并且可以通过将其重命名回正常来取消删除。当隐藏文件从命名空间中删除时，其内存中的元数据被擦除。这有效地切断了它与其所有chunk的链接。

在类似的常规扫描chunk命名空间中，master识别孤立chunk(不可从任何文件访问的chunk)并擦除这些chunk的元数据。在与master的定期HeartBeat消息中，每个chunkserver报告它拥有的chunk子集，master回复不再存在于master元数据中的任何chunk的身份。chunkserver可以自由删除此类chunk的副本。

#### 4.4.2 讨论

虽然分布式垃圾收集是一个困难的问题，需要复杂的解决方案在编程语言的上下文中，但在我们的情况下相对简单。我们可以轻松识别对chunk的所有引用：它们在master专门维护的文件到chunk映射中。我们还可以轻松识别所有chunk副本：它们是每个chunkserver上指定目录下的Linux文件。master不知道的任何此类副本都是"垃圾"。

垃圾收集方法相对于急切删除有几个优势。首先，在组件故障常见的大规模分布式系统中，它简单可靠。chunk创建可能在某些chunkserver上成功，在其他chunkserver上失败，留下master不知道的副本。副本删除消息可能丢失，master必须记住重新发送它们，包括在自己的故障和重新启动中。垃圾收集提供了一种统一可靠的方式来清理任何不知道的副本。其次，它将存储回收合并到master的常规后台活动中，例如命名空间的常规扫描和与chunkserver的HandShake。因此，它是批量完成的，成本被摊销。此外，它只在master相对空闲时完成。master可以更快地响应需要及时关注的客户端请求。第三，延迟存储回收为意外、不可逆删除提供了安全网。

根据我们的经验，主要缺点是延迟有时会阻碍用户努力微调存储使用，当存储紧张时。重复创建和删除临时文件的应用程序可能无法立即重用存储。我们通过在文件再次被删除时加速存储回收，以及允许用户对命名空间的不同部分应用不同的复制和回收策略来解决这些问题。例如，用户可以指定某些目录树中的所有文件都存储而不复制，并且任何删除的文件都立即且不可逆地从文件系统状态中删除。

### 4.5 陈旧副本检测

如果chunkserver失败并错过对chunk的变更，chunk副本可能变得陈旧。对于每个chunk，master维护一个chunk版本号以区分最新和陈旧的副本。

每当master将新租约授予chunk时，它都会增加chunk版本号并通知最新副本。master和这些副本都在其持久状态中记录新版本号。这发生在任何客户端被通知之前，因此在开始写入chunk之前。如果另一个副本当前不可用，其chunk版本号将不会前进。当chunkserver重新启动并向master报告其chunk集及其关联版本号时，master将检测到该chunkserver具有陈旧副本。如果master看到比其记录中更高的版本号，master假设它在授予租约时失败，因此将更高版本作为最新版本。

master在其常规垃圾收集中删除陈旧副本。在此之前，当回复客户端对chunk信息的请求时，它有效地认为陈旧副本根本不存在。作为另一个保障，master在向客户端提供chunk位置信息或指示chunkserver从另一个chunkserver克隆chunk时包括chunk版本号。客户端或chunkserver在执行操作时验证版本号，以便它们始终访问最新数据。

## 5 容错和诊断

我们面临的最大挑战之一是处理频繁的组件故障。组件的质量和数量使得这些问题更多是常态而不是例外：我们不能完全信任机器，也不能完全信任磁盘。组件故障可能导致系统不可用或更糟糕的是，损坏数据。我们讨论我们如何应对这些挑战以及我们构建到系统中的工具，以便在问题不可避免地出现时诊断问题。

### 5.1 高可用性

在GFS集群中，数百个服务器中，在任何给定时间，一些肯定是关闭的。我们通过两个简单而有效的策略保持整个系统高度可用：快速恢复和复制。

#### 5.1.1 快速恢复

master和chunkserver都被设计为在几秒钟内恢复其状态并启动，无论它们如何终止。实际上，我们不区分正常和异常终止；服务器通过简单地终止进程来例行关闭。客户端和其他服务器在其未完成的请求超时时经历小的打嗝，重新连接到重新启动的服务器，并重试。第6.2.2节报告了观察到的启动时间。

#### 5.1.2 Chunk复制

如前所述，每个chunk都在不同机架上的多个chunkserver上复制。用户可以为文件命名空间的不同部分指定不同的复制级别。默认值为三个副本。当chunkserver离线或通过校验和检测到损坏的副本时(参见第5.2节)，master克隆现有副本以保持每个chunk完全复制。虽然复制为我们提供了可靠性，但我们也在探索其他形式的跨服务器冗余，如奇偶校验或纠删码，以满足我们不断增长的只读存储需求。我们认为在我们松散耦合的系统中实现这些更复杂的冗余方案是具有挑战性但可管理的，其中数千个磁盘分布在数百台机器上。

#### 5.1.3 Master复制

master状态被复制以实现可靠性。其操作日志和检查点被复制到多台机器上。只有在将相应的日志记录刷新到本地磁盘以及所有master副本之后，对其状态的变更才被认为是提交的。为了简单起见，一个master进程负责所有变更以及后台活动，如垃圾收集。当它失败时，它可以几乎立即重新启动。如果其机器或磁盘失败，集群外的监控基础设施将在其他地方启动新的master进程，并访问复制的操作日志。客户端仅使用master的规范名称(例如gfs-test)，这是一个DNS别名，如果master重新定位到另一台机器，可以更改。

此外，"影子"master提供只读访问文件系统，即使主master关闭。它们是影子，不是镜像，因为它们可能稍微落后于主master，通常是几分之一秒。它们增强了不需要变更文件系统或不介意获取稍微陈旧信息的应用程序的读取可用性。实际上，由于文件内容从chunkserver读取，应用程序不会观察到陈旧的文件内容。在短窗口内可能陈旧的是文件元数据，如目录内容或访问控制信息。

为了保持自己的更新，影子master读取不断增长的操作日志的副本，并以与主master完全相同的顺序应用相同的更改序列到其数据结构。像主master一样，它在启动时轮询chunkserver(并且此后很少)以定位chunk副本，并通过定期与它们握手来监控chunkserver状态。它仅依赖于主master进行副本放置决策。

### 5.2 数据完整性

每个chunkserver使用校验和来检测存储数据的损坏。考虑到GFS集群通常有数百台机器上的数千个磁盘，它经常经历磁盘故障，这些故障可能导致数据损坏或丢失(参见第7节了解我们经历的故障类型)。我们可以使用其他chunk副本来恢复损坏，但比较跨副本来检测损坏是不切实际的。此外，分歧副本可能是合法的：GFS变更的语义，特别是如前所述的原子记录追加，不保证副本在字节上相同。因此，每个chunkserver必须通过维护校验和独立验证其自己副本的完整性。

chunk被分解为64KB块。每个都有相应的32位校验和。像其他元数据一样，校验和保存在内存中并与日志记录一起持久存储，与用户数据分开。

对于读取，chunkserver在将任何数据返回给请求者(无论是客户端还是另一个chunkserver)之前验证与读取范围重叠的数据块的校验和。因此，chunkserver不会将损坏传播到其他机器。如果块不匹配记录的校验和，chunkserver将错误返回给请求者，并向master报告不匹配。作为响应，请求者将从其他副本读取，而master将从另一个副本克隆chunk。在有效的新副本就位后，master指示报告不匹配的chunkserver删除其副本。

校验和对读取性能的影响很小，原因有几个。由于我们的大多数读取跨越多个块，我们只需要读取和验证相对少量的额外数据进行验证。GFS客户端代码通过尝试在块边界上对齐其读取来进一步减少此开销。此外，chunkserver上的校验和查找和比较是在没有任何I/O的情况下完成的，校验和计算通常可以与I/O重叠。

校验和计算针对追加到chunk末尾的写入(而不是覆盖现有数据的写入)进行了大量优化，因为它们在我们的工作负载中占主导地位。我们只是增量更新最后一个部分块的校验和，并为追加填充的任何全新块计算新校验和。即使最后一个部分块已经损坏并且我们现在无法检测到它，新的校验和值也不会匹配存储的数据，当下次读取块时将检测到损坏。

相比之下，如果写入覆盖chunk的现有范围，我们必须读取和验证被覆盖范围的第一个和最后一个块，然后执行写入，最后计算并记录新校验和。如果我们在覆盖之前不验证第一个和最后一个部分块，新校验和可能隐藏在未被覆盖的区域中存在的损坏。

在空闲期间，chunkserver可以扫描和验证不活动chunk的内容。这允许我们检测很少读取的chunk中的损坏。一旦发现损坏，master可以创建新的未损坏副本并删除损坏的副本。这防止不活动但损坏的chunk副本欺骗master认为它有足够的chunk有效副本。

### 5.3 诊断工具

广泛而详细的诊断日志记录在问题隔离、调试和性能分析方面提供了无价的帮助，而成本很小。没有日志，很难理解机器之间的瞬态、不可重现的交互。GFS服务器生成诊断日志，记录许多重要事件(如chunkserver上线和下线)以及所有RPC请求和回复。这些诊断日志可以自由删除而不影响系统的正确性。但是，我们尽量保留这些日志，只要空间允许。

RPC日志包括发送到网络的确切请求和回复，除了读取和写入的文件数据。通过匹配请求与回复并整理不同机器上的RPC记录，我们可以重建整个交互历史来诊断问题。日志还用作负载测试和性能分析的跟踪。

日志记录的性能影响很小(通常远小于1%)，因为这些日志是顺序和异步写入的。最近的事件也保存在内存中，可用于连续在线监控。

## 6 测量

在本节中，我们提供一些微基准测试来说明GFS架构和实现中固有的瓶颈，以及来自真实集群的一些数字。

### 6.1 微基准测试

我们在由一个master、两个master副本、16个chunkserver和16个客户端组成的GFS集群上测量了性能。请注意，这种配置是为了便于测试，典型的集群有数百个chunkserver和数百个客户端。

所有机器都配置有双1.4GHz PIII处理器、2GB RAM、两个80GB 5400rpm磁盘和100Mbps全双工以太网连接到HP 2524交换机。所有19个GFS服务器机器都连接到一个交换机，所有16个客户端机器都连接到另一个交换机。两个交换机通过1Gbps链路连接。

#### 6.1.1 读取

图3(a)显示了N个客户端从文件系统同时读取的总读取速率。每个客户端从320GB文件集中读取4MB区域的随机选择序列。这被重复300次，因此每个客户端最终读取1.2GB。chunkserver总共只有32GB内存，因此我们期望最多1%的命中率在Linux缓冲区缓存中。我们的结果应该接近从磁盘读取的性能。

单个客户端的读取速率为10MB/s，或100Mbps网络连接的80%。16个客户端的总读取速率达到94MB/s，即每个客户端6MB/s，或其100Mbps网络连接的47%。效率从80%下降到47%的原因是我们接近1Gbps交换机间链路的极限，该链路在客户端和chunkserver之间共享。

#### 6.1.2 写入

图3(b)显示了N个客户端同时写入N个不同文件的总写入速率。每个客户端以1MB的块写入1GB到以前不存在的文件。总写入速率达到16个客户端的67MB/s，即每个客户端4.2MB/s，约为其100Mbps网络连接的33%。

这比读取情况更糟，主要有三个原因。首先，写入速率受到将数据写入三个副本的chunkserver的限制。对于我们的网络拓扑，客户端必须通过交换机间链路将数据推送到两个机架中的chunkserver，从而将每个客户端的有效带宽减少到50Mbps。其次，我们没有优化跨多个副本推送数据。我们将数据推送到副本链而不是并行分布到所有副本。第三，我们的冲突避免策略(参见第6.3.1节)保守地减少了一些客户端的写入速率。

#### 6.1.3 记录追加

图3(c)显示了记录追加的性能。N个客户端同时追加到单个文件。性能受到存储文件最后一个chunk的chunkserver的网络带宽限制，与客户端数量无关。它从一个客户端的6.3MB/s开始，下降到16个客户端的4.8MB/s，主要是由于不同客户端的网络拥塞和各种网络堆栈中的差异。

我们的应用程序倾向于生成多个此类文件同时。换句话说，N个客户端同时追加到M个共享文件，其中N和M都是数十或数百。因此，在我们的实验中，chunkserver网络拥塞不是我们系统中的重大瓶颈，因为客户端可以在许多文件之间取得进展。

### 6.2 真实世界集群

我们现在检查在Google广泛使用的两个GFS集群。集群A由研究和开发使用，集群B主要由生产数据处理使用。表2显示了这些集群的特征。

#### 6.2.1 存储

两个集群都有数百个chunkserver。集群B有2.5倍的chunkserver和5倍的磁盘空间。在这两个集群中，超过75%的chunk是满的64MB chunk。

两个集群中的大多数文件都很小，但大文件占大部分字节。集群B有大约100,000个文件，其中50,000个大于1GB，27,000个大于4GB。相比之下，集群A有类似数量的文件，但只有17,000个大于1GB，1,300个大于4GB。

#### 6.2.2 元数据

chunkserver总共存储数十GB的元数据，主要是每个64KB块的64字节校验和。其余元数据包括chunk版本信息和各种簿记。

master上的元数据要小得多，每个文件平均只有几百字节。这与我们的假设一致，即系统的容量不会受到master保持所有元数据在内存中的能力的根本限制。大多数每个文件的元数据是以前缀压缩形式的文件名。其他元数据包括文件所有权和权限、从文件到chunk的映射，以及每个chunk的当前版本。此外，对于每个chunk，master存储当前副本位置和引用计数，用于实现写时复制。

每个单独的服务器，包括chunkserver和master，通常只有50到100MB的元数据。

#### 6.2.3 读写速率

表3显示了在一周内不同时间的各种操作的速率。两个集群在一周内都维持了持续的高活动水平。

集群B比集群A有更高的读取速率，因为前者支持更多的生产活动。如预期的那样，两个集群的写入速率都比读取速率低。集群A每秒有300次写入，生成50MB/s的数据，而集群B每秒有100次写入，生成25MB/s的数据。这些写入速率满足我们应用程序的需求。在实践中，写入往往是突发的：应用程序写入1GB文件，然后暂停，然后写入另一个文件。

记录追加速率比写入速率高几个数量级。集群A和B分别看到每秒1,000和300次记录追加。对于集群A，这转化为40MB/s的数据，而对于集群B，这转化为18MB/s的数据。这些速率再次满足我们应用程序的期望，这些应用程序很少依赖于单个文件的追加吞吐量，而是关心系统的总吞吐量和可靠性。

#### 6.2.4 Master工作负载

表4显示了发送到master的请求的细分。大多数请求询问chunk位置(FindLocation)和读取元数据(FindLeaseLocker，它查询哪个chunkserver持有chunk的租约)。

集群A和B看到每秒200到500个操作，这很容易由单个master处理。然而，master当前的回复速率约为每秒1,000个操作，因此它不是当前性能的瓶颈。

#### 6.2.5 恢复时间

当chunkserver失败时，一些chunk将变得复制不足，必须从其余副本克隆。恢复到完全复制状态所需的时间取决于资源量。在我们的一个实验中，我们杀死了集群B中的单个chunkserver。该chunkserver有大约15,000个chunk，包含600GB的数据。为了限制对运行应用程序的影响并提供调度灵活性，我们的默认参数将此类克隆操作的数量限制为91(集群中chunkserver数量的40%)，每个克隆操作限制为6.25MB/s。所有chunk在23.2分钟内恢复，有效复制速率为440MB/s。

在另一个实验中，我们杀死了两个chunkserver，每个都有大约16,000个chunk和660GB的数据。这两个故障将266个chunk减少到单个副本。这266个chunk以更高的优先级克隆，并在2分钟内恢复到至少2倍复制，有效复制速率为483MB/s。

## 7 经验

在构建和部署GFS的过程中，我们经历了各种问题，有些是操作性的，有些是技术性的。

最初，GFS被构想为我们生产系统的后端文件系统。随着时间的推移，使用已经增长到包括研究和开发任务。这改变了一些我们的原始假设。例如，我们原本没有预期到应用程序bug、用户错误、操作员错误或磁盘故障之外的数据损坏。

虽然磁盘故障是我们预期的常见原因，但我们现在看到更多由于应用程序bug导致的数据损坏。这种观察激发了校验和的添加。一些应用程序错误是我们无法合理地期望GFS本身检测到的——例如，写入错误数据，或在正确时间之前写入数据。我们考虑了各种校验和方案，但决定应用程序最好执行端到端完整性检查。另一方面，我们已经发现校验和对于检测chunkserver中的磁盘或IDE子系统中的损坏非常有用。

在最初的设计中，我们设想GFS主要用于大型流式读取和文件追加。实际上，它也被用于许多小型随机读取。我们通过增加master的副本数量和允许应用程序从副本读取只读数据来优化此工作负载，从而分散负载。

另一个问题是我们使用Linux单线程fsck需要太长时间来恢复大文件系统，特别是当文件系统在chunkserver启动期间被损坏时。我们通过更改磁盘格式来允许多个fsck同时在不同部分的文件系统上运行来解决这个问题。

最初，我们有一个简单的垃圾收集策略：在文件删除后的固定时间间隔后删除任何副本。我们发现这对于管理存储使用是不够灵活的，特别是当我们的集群使用模式改变时。我们现在公开控制，允许应用程序通过设置不同的复制和垃圾收集策略来调整其存储使用。例如，应用程序可以指定某些文件应该存储而不复制，某些文件应该在删除后立即删除而不是延迟垃圾收集。

最后，我们最初低估了某些网络堆栈问题的重要性。例如，我们花了一些时间意识到Linux TCP堆栈中的错误导致在高负载下数据损坏。这些问题很难跟踪，因为损坏很少，并且通常表现为应用程序中的校验和不匹配而不是内核崩溃。我们通过在GFS中使用校验和来检测此类问题，并通过修改内核来处理这些Linux问题。现在我们更喜欢使用CRC而不是TCP或以太网提供的校验和。

## 8 相关工作

像许多其他大规模分布式文件系统一样，GFS提供了一个位置透明的命名空间，允许数据在集群中重新分布以实现负载平衡或容错。与早期的分布式文件系统不同，GFS将文件系统接口与POSIX等标准分离，以简化整个系统。

我们的工作与Andrew文件系统(AFS)有一些相似之处，因为AFS将"回调"用于缓存一致性，我们使用租约。但是，与AFS不同，我们不提供分布式文件系统缓存，我们针对大文件和顺序工作负载进行了优化。

分布式对象存储系统如Bayou、Coda、Ficus和InterMezzo解决了分布式文件系统面临的许多相同问题，特别是各种故障模式下的可用性。与这些系统不同，我们的威胁模型只包括磁盘或机器故障，不包括恶意攻击，我们针对大文件进行了优化，这些文件大多通过追加而不是随机写入进行修改。

在解决单点故障和提高可扩展性方面，一些文件系统已经消除了中央服务器。例如，在xFS中，分布式服务器的合作联盟管理所有文件系统元数据。我们采用了一个集中式方法来简化设计，增加可靠性，并获得灵活性。特别是，集中式master使得复杂的chunk放置和复制策略变得更加容易。我们通过保持master状态小并将其复制到多台机器来解决可用性和性能问题。

GFS与Lustre最相似，因为两者都有集群存储系统，具有单个元数据服务器。但是，Lustre针对传统POSIX文件系统语义和相对较小的文件进行了优化，而GFS针对我们的应用程序工作负载和大文件进行了优化。

GFS的容错策略与Harp和Petal等系统不同。我们通过在商品级chunkserver上复制数据来实现高可用性，而这些系统提供RAID样式的可靠性，仅在文件服务器级别。与我们的系统相比，Petal提供了更低级别的原始磁盘接口，而Harp提供了传统的文件系统接口。

## 9 结论

Google文件系统演示了如何支持大规模分布式应用程序在商品硬件上的存储需求。虽然一些设计决策是特定于我们独特设置的，但许多其他决策适用于类似规模和成本意识的数据处理任务。

我们将常见情况视为设计的中心。我们的工作负载和技术环境继续发展，但我们相信许多假设，如大文件、变更主要通过追加、高持续带宽比低延迟更重要，将保持相关，我们的设计决策将继续有效。

GFS成功地满足了我们的存储需求，并广泛部署在Google内部作为我们服务的存储平台以及研究和开发工作。它是一个重要的工具，使我们能够继续创新和解决有趣的问题，规模是我们以前无法尝试的。

---

**译者注**: 本文翻译自Google发表的GFS经典论文，GFS作为Google大数据基础设施的重要组成部分，为后续的HDFS等分布式文件系统提供了重要的设计思路和实践经验。GFS的设计理念和架构对整个分布式存储领域产生了深远影响。