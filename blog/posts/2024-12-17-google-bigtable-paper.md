---
title: "Google BigTable论文：结构化数据的分布式存储系统"
date: "2024-12-17"
author: "Google Research Team"
tags: ["BigTable", "分布式存储", "Google", "大数据", "论文翻译", "数据库"]
category: "大数据"
excerpt: "Google BigTable是一个管理结构化数据的分布式存储系统，旨在扩展到非常大的规模：跨数千台商用服务器的PB级数据。本文详细介绍了BigTable的设计理念、数据模型、系统架构以及在Google内部的实际应用案例。"
---

# BigTable: 结构化数据的分布式存储系统

## 摘要

BigTable是一个管理结构化数据的分布式存储系统，旨在扩展到非常大的规模：跨数千台商用服务器的PB级数据。Google的许多项目将数据存储在BigTable中，包括Web索引、Google Earth和Google Finance。这些应用对BigTable提出了截然不同的需求：数据大小（从URL到网页到卫星图像）和延迟要求（从后端批处理到实时数据服务）都有很大差异。尽管需求多样，BigTable已经成功地为所有这些Google产品提供了灵活、高性能的解决方案。

本文描述了BigTable提供的简单数据模型，该模型使客户端能够动态控制数据的布局和格式，并描述了BigTable的设计和实现。

## 1. 引言

在过去的两年半中，我们设计、实现并部署了一个分布式存储系统，用于管理Google的结构化数据，名为BigTable。BigTable旨在可靠地扩展到PB级数据和数千台机器。BigTable已经实现了几个目标：广泛的适用性、可扩展性、高性能和高可用性。

BigTable在Google内部被60多个产品和项目使用，包括Google Analytics、Google Finance、Orkut、个性化搜索、Writely和Google Earth。这些产品使用BigTable处理各种工作负载，从面向吞吐量的批处理作业到对最终用户延迟敏感的数据服务。用于这些产品的BigTable集群的配置差异很大，从几台服务器到数千台服务器，存储多达数百TB的数据。

在许多方面，BigTable类似于数据库：它与数据库共享许多实现策略。并行数据库和主内存数据库已经实现了可扩展性和高性能，但BigTable提供了与此类系统不同的接口。BigTable不支持完整的关系数据模型；相反，它为客户端提供了一个简单的数据模型，支持对数据布局和格式的动态控制，并允许客户端推理底层存储中数据的位置属性。数据使用行和列名称进行索引，这些名称可以是任意字符串。BigTable还将数据视为未解释的字符串，尽管客户端通常将各种形式的结构化和半结构化数据序列化到这些字符串中。客户端可以通过仔细选择其数据的模式来控制数据的位置。最后，BigTable模式参数允许客户端动态控制是从磁盘还是从内存提供数据。

## 2. 数据模型

BigTable是一个稀疏的、分布式的、持久的多维排序映射。映射由行键、列键和时间戳索引；每个值都是一个未解释的字节数组。

```
(row:string, column:string, time:int64) → string
```

### 2.1 行

表中的行键是任意字符串（目前最大64KB，尽管通常10-100字节）。对单行的每次读取或写入都是原子的（无论该行中读取或写入多少不同的列），这一设计决策使客户端更容易推理并发更新同一行时的系统行为。

BigTable按行键的字典顺序维护数据。表中的行范围被动态分区。每个行范围称为tablet，它是分布和负载平衡的单位。因此，读取短行范围是高效的，通常只需要与少数机器通信。客户端可以利用这个属性，通过选择行键使得对相关数据的访问通常访问少数机器。

### 2.2 列族

列键被分组为称为列族的集合，这形成了访问控制的基本单位。存储在同一列族中的所有数据通常属于同一类型（我们将同一列族中的数据压缩在一起）。在可以将数据存储在列族中之前，必须创建列族；创建列族后，可以使用族内的任何列键。我们的意图是表中不同列族的数量应该很小（最多几百个），并且族在操作期间很少更改。相比之下，表可能有无限数量的列。

列键使用以下语法命名：`family:qualifier`。列族名称必须是可打印的，但限定符可以是任意字符串。

### 2.3 时间戳

BigTable中的每个单元格可以包含同一数据的多个版本；这些版本由时间戳索引。BigTable时间戳是64位整数。它们可以由BigTable分配，在这种情况下，它们表示"实时"微秒，或者由客户端应用程序显式分配。必须生成唯一时间戳的应用程序必须避免冲突本身。单元格中的不同版本按时间戳降序存储，以便首先读取最新版本。

为了使管理版本化数据的负担不会过重，我们支持两个每列族设置，告诉BigTable自动垃圾收集单元格版本。客户端可以指定应该保留数据的最后n个版本，或者只保留足够新的版本（例如，只保留最后七天写入的值）。

## 3. API

BigTable API提供了创建和删除表和列族的功能。它还提供了更改集群、表和列族元数据（如访问控制权限）的功能。

客户端应用程序可以写入或删除BigTable中的值，从单个行查找值，或者迭代表中数据的子集。

BigTable支持几个其他功能，允许用户以更复杂的方式操作数据。首先，BigTable支持单行事务，可用于对存储在单行下的数据执行原子读-修改-写序列。BigTable目前不支持跨行的通用事务，尽管它为客户端提供了跨行批量写入的接口。其次，BigTable允许将单元格用作整数计数器。最后，BigTable支持在服务器地址空间中执行客户端提供的脚本。脚本用Google开发的用于处理数据的语言Sawzall编写。

BigTable可以与MapReduce一起使用，MapReduce是Google开发的用于大规模并行计算的框架。我们编写了一组包装器类，允许将BigTable用作MapReduce作业的输入和输出。

## 4. 构建块

BigTable建立在其他几个Google基础设施组件之上。BigTable使用分布式Google文件系统（GFS）来存储日志和数据文件。BigTable集群通常在共享机器池中运行，该池运行各种其他分布式应用程序，BigTable进程通常与其他应用程序的进程共享机器。BigTable依赖于集群管理系统来调度作业、管理共享机器上的资源、处理机器故障并监控机器状态。

Google SSTable文件格式在内部用于存储BigTable数据。SSTable提供了从键到值的持久、有序、不可变映射，其中键和值都是任意字节字符串。提供了查找与特定键关联的值以及迭代指定键范围内所有键值对的操作。在内部，每个SSTable包含一系列块（通常每个块64KB，但这是可配置的）。块索引（存储在SSTable的末尾）用于定位块；当打开SSTable时，索引被加载到内存中。查找可以通过单个磁盘搜索执行：我们首先使用二进制搜索在内存中的索引中找到适当的块，然后从磁盘读取适当的块。可选地，SSTable可以完全映射到内存中，这允许我们执行查找和扫描而无需触及磁盘。

BigTable还依赖于一个高度可用和持久的分布式锁服务，称为Chubby。Chubby服务由五个活动副本组成，其中一个被选为主副本并主动服务请求。只有当大多数副本正在运行并且可以相互通信时，服务才是活动的。Chubby使用Paxos算法在面对故障时保持其副本的一致性。Chubby提供了一个由目录和小文件组成的命名空间。每个目录或文件都可以用作锁，对文件的读写是原子的。Chubby客户端库提供Chubby文件的一致缓存。每个Chubby客户端维护与Chubby服务的会话。如果客户端无法在租约到期时间内续订其会话租约，则客户端的会话到期。当客户端的会话到期时，它会丢失任何锁和打开的句柄。Chubby客户端还可以在Chubby文件和目录上注册回调，以便在更改或会话到期时收到通知。

BigTable使用Chubby执行各种任务：确保在任何给定时间最多有一个活动主副本；存储BigTable数据的引导位置；发现tablet服务器并完成它们的死亡；存储BigTable模式信息（每个表的列族信息）；以及存储访问控制列表。如果Chubby长时间不可用，BigTable将变得不可用。我们最近在14个BigTable集群中测量了这种影响，平均每个集群有11台机器。由于Chubby不可用（由Chubby本身的故障或网络问题引起），BigTable数据的平均不可用时间百分比为0.0047%。单个集群中最长的Chubby不可用期间持续了5.2小时。

## 5. 实现

BigTable实现有三个主要组件：链接到每个客户端的库、一个主服务器和许多tablet服务器。可以根据工作负载的变化动态添加（或删除）tablet服务器到集群。

主服务器负责将tablet分配给tablet服务器，检测tablet服务器的添加和到期，平衡tablet服务器负载，以及垃圾收集GFS中的文件。此外，它处理模式更改，如表和列族创建。

每个tablet服务器管理一组tablet（通常每个服务器有十到一千个tablet）。tablet服务器处理对其加载的tablet的读写请求，并在tablet变得太大时拆分tablet。

与许多单主分布式存储系统一样，客户端数据不通过主服务器移动：客户端直接与tablet服务器通信进行读写。因为BigTable客户端不依赖主服务器获取tablet位置信息，大多数客户端从不与主服务器通信。因此，主服务器在实践中负载很轻。

BigTable集群存储许多表。每个表由一组tablet组成，每个tablet包含与行范围关联的所有数据。最初，每个表只包含一个tablet。随着表的增长，它会自动拆分为多个tablet，默认情况下每个tablet大约为100-200 MB。

### 5.1 Tablet位置

我们使用三级层次结构（类似于B+树）来存储tablet位置信息。

第一级是存储在Chubby中的文件，其中包含根tablet的位置。根tablet包含特殊METADATA表中所有tablet的位置。每个METADATA行存储用户tablet的位置。根tablet只是METADATA表的第一个tablet，但它被特殊处理——它从不拆分——以确保tablet位置层次结构不超过三级。

METADATA表将每个tablet的位置存储在由tablet的表标识符和其结束行编码的行键下。METADATA表中的每一行在内存中存储大约1KB的数据。使用适度的128 MB METADATA tablet限制，我们的三级位置方案足以寻址2^34个tablet（或2^61字节在128 KB tablet中）。

客户端库缓存tablet位置。如果客户端不知道tablet的位置，或者发现其缓存的位置信息不正确，则它会递归地向上移动tablet位置层次结构。如果客户端的缓存为空，则位置算法需要三次网络往返，包括一次从Chubby读取。如果客户端的缓存过时，位置算法可能需要多达六次往返，因为过时的缓存条目只有在未命中时才会被发现（假设METADATA tablet不经常移动）。尽管tablet位置存储在内存中，因此不需要GFS访问，但我们通过让客户端库预取tablet位置来进一步减少此开销：每当它读取METADATA表时，它会读取多个tablet的元数据。

我们还在METADATA表中存储次要信息，包括每个tablet的事件日志（用于调试和性能分析）。

### 5.2 Tablet分配

每个tablet在任何给定时间最多分配给一个tablet服务器。主服务器跟踪活动tablet服务器的集合以及当前分配给tablet服务器的tablet，包括哪些tablet未分配。当未分配的tablet存在且有足够容量的tablet服务器可用时，主服务器通过向tablet服务器发送tablet加载请求来分配tablet。

BigTable使用Chubby跟踪tablet服务器。当tablet服务器启动时，它会在特定Chubby目录中创建并获取唯一命名文件的独占锁。主服务器监控此目录（服务器目录）以发现tablet服务器。如果tablet服务器失去其独占锁——例如，由于网络分区导致tablet服务器失去其Chubby会话——则它会停止为其tablet提供服务。（Chubby提供了一种高效的机制，允许tablet服务器检查它是否仍然持有其锁而不会产生网络流量。）只要文件仍然存在，tablet服务器就会尝试重新获取其文件的独占锁。如果文件不再存在，则tablet服务器将永远无法再次提供服务，因此它会自杀。每当tablet服务器终止时（例如，因为集群管理系统正在从其机器中删除tablet服务器），它会尝试释放其锁，以便主服务器可以更快地重新分配其tablet。

主服务器负责检测tablet服务器何时不再为其tablet提供服务，并尽快重新分配这些tablet。为了检测tablet服务器何时不再为其tablet提供服务，主服务器定期询问每个tablet服务器其锁的状态。如果tablet服务器报告它已失去其锁，或者如果主服务器无法在几次尝试中到达服务器，则主服务器尝试获取该服务器文件的独占锁。如果主服务器能够获取锁，则Chubby是活动的，tablet服务器要么已死要么在到达Chubby时遇到问题，因此主服务器通过删除其服务器文件来确保tablet服务器永远无法再次提供服务。一旦服务器的文件被删除，主服务器就可以将之前分配给该服务器的所有tablet移动到未分配tablet的集合中。为了确保BigTable集群不容易受到主服务器和Chubby之间网络问题的影响，如果主服务器的Chubby会话到期，主服务器会自杀。但是，如上所述，主服务器故障不会更改现有tablet到tablet服务器的分配。

当集群管理系统启动主服务器时，主服务器需要发现当前的tablet分配，然后才能更改它们。主服务器在启动时执行以下步骤：(1) 主服务器从Chubby获取唯一的主锁，这可以防止并发主实例化。(2) 主服务器扫描Chubby中的服务器目录以查找活动服务器。(3) 主服务器与每个活动tablet服务器通信以发现已分配给每个服务器的tablet集合。(4) 主服务器扫描METADATA表以了解tablet集合。每当此扫描遇到尚未分配的tablet时，主服务器将tablet添加到未分配tablet的集合中以便进行分配。

可能出现的一个复杂情况是，在主服务器完成步骤4之前，无法扫描METADATA表，因为根tablet可能尚未分配。因此，主服务器在开始此扫描之前将根tablet添加到未分配tablet的集合中（如果在步骤3中未发现根tablet已分配）。此添加确保根tablet将被分配。因为根tablet包含所有METADATA tablet的名称，所以主服务器在扫描根tablet后知道所有这些名称。

现有tablet的集合仅在创建或删除表时、两个现有tablet合并形成一个更大的tablet时或现有tablet拆分为两个更小的tablet时才会更改。主服务器能够跟踪这些更改，因为除了最后一个之外，它启动了所有更改。Tablet拆分得到特殊处理，因为它们由tablet服务器启动。tablet服务器通过在METADATA表中记录新tablet的信息来提交拆分。当拆分提交时，它会通知主服务器。如果拆分通知丢失（要么因为tablet服务器要么主服务器已死），主服务器在要求tablet服务器加载已拆分的tablet时会检测到新tablet。tablet服务器将通知主服务器拆分，因为它在METADATA表中找到的tablet条目将仅指定主服务器要求它加载的tablet的一部分。

### 5.3 Tablet服务

tablet的持久状态存储在GFS中。更新提交到存储在GFS中的提交日志。在这些更新中，最近的更新存储在内存中的排序缓冲区中，称为memtable；较旧的更新存储在一系列SSTable中。为了恢复tablet，tablet服务器从METADATA表中读取其元数据。此元数据包含组成tablet的SSTable列表以及一组重做点，这些重做点是指向可能包含tablet数据的任何提交日志的指针。服务器将SSTable的索引读入内存，并通过应用自重做点以来提交的所有更新来重构memtable。

当对tablet服务器执行写操作时，服务器检查操作是否格式良好，以及发送者是否有权执行突变。授权通过从Chubby文件中读取允许写入者列表来执行（该文件几乎总是在Chubby客户端缓存中命中）。有效的突变被写入提交日志。可以使用组提交来提高大量小突变的吞吐量。提交写操作后，其内容将插入到memtable中。

当对tablet服务器执行读操作时，类似地检查操作是否格式良好和正确授权。有效的读操作在合并的SSTable和memtable视图上执行。由于SSTable和memtable是按字典顺序排序的数据结构，因此可以高效地形成合并视图。

传入的读写操作可以在tablet拆分和合并发生时继续。

### 5.4 压缩

随着写操作的执行，memtable的大小增加。当memtable大小达到阈值时，memtable被冻结，创建新的memtable，冻结的memtable被转换为SSTable并写入GFS。这种次要压缩过程有两个目标：它缩小了tablet服务器的内存使用量，并且在服务器死亡的情况下减少了必须从提交日志中读取以进行恢复的数据量。传入的读写操作可以在压缩发生时继续。

每个次要压缩都会创建一个新的SSTable。如果这种行为继续不受检查，读操作可能需要合并来自任意数量的SSTable的更新。相反，我们通过在后台定期执行合并压缩来限制此类文件的数量。合并压缩读取一些SSTable和memtable的内容，并写出新的SSTable。一旦压缩完成，输入SSTable和memtable就可以被丢弃。

重写所有SSTable的合并压缩称为主要压缩。由非主要压缩产生的SSTable可以包含特殊的删除条目，这些条目抑制较旧SSTable中仍然存在的已删除数据。另一方面，主要压缩产生的SSTable不包含删除信息或已删除数据。BigTable定期循环其所有tablet的主要压缩。这些主要压缩允许BigTable回收已删除数据使用的资源，并确保已删除数据及时从系统中消失，这对于存储敏感数据的服务很重要。

## 6. 细化

前面的部分描述了BigTable实现的基本架构。本节描述了BigTable实现的部分，这些部分需要更详细的解释。

### 6.1 局部性组

客户端可以将多个列族分组为局部性组。为每个tablet中的每个局部性组生成单独的SSTable。将通常不一起访问的列族分离到不同的局部性组中可以实现更高效的读取。例如，Webtable中的页面元数据（如语言和校验和）可以在一个局部性组中，页面内容可以在另一个局部性组中：想要读取元数据的应用程序不需要读取所有页面内容。

此外，可以为每个局部性组指定一些有用的调整参数。例如，可以将局部性组声明为驻留内存。驻留内存局部性组的SSTable被延迟加载到tablet服务器的内存中。一旦加载，属于这些局部性组的列族就可以被读取而无需访问磁盘。此功能对于小块经常访问的数据很有用：我们在内部将其用于METADATA表中的位置列族。

### 6.2 压缩

客户端可以控制是否压缩局部性组的SSTable，如果是，应该使用哪种压缩格式。用户指定的压缩格式应用于每个SSTable块（其大小可通过局部性组特定的调整参数控制）。尽管分别压缩每个块会丢失一些空间，但我们受益于能够读取SSTable的小部分而无需解压缩整个文件。许多客户端使用两遍自定义压缩方案。第一遍使用Bentley和McIlroy的方案，该方案在大窗口中压缩长公共字符串。第二遍使用快速压缩算法，在16 KB窗口中查找重复。两个压缩遍都非常快——它们以100–200 MB/s的速度编码，以400–1000 MB/s的速度解码。

即使用户选择不使用压缩，我们的两遍压缩方案的性能也令人印象深刻。例如，在Webtable中，我们使用此压缩方案将网页本身存储在一个局部性组中，将锚点存储在另一个局部性组中。在网页局部性组中，我们实现了10:1的压缩比。这比典型的Gzip压缩比要好得多，因为Webtable行的布局方式：来自同一主机的所有页面都存储在附近。这允许Bentley-McIlroy算法识别来自同一主机的页面中的大量共享样板。许多应用程序，而不仅仅是Webtable，选择其行名称，以便类似的数据最终聚集在一起，因此实现了非常好的压缩比。当我们在Webtable中存储相同语言的网页时，压缩比甚至更好。

### 6.3 通过缓存提高读性能

为了提高读性能，tablet服务器使用两级缓存。扫描缓存是更高级别的缓存，它缓存tablet服务器代码返回给客户端的键值对。块缓存是较低级别的缓存，它缓存从GFS读取的SSTable块。扫描缓存对于倾向于重复读取相同数据的应用程序最有用。块缓存对于倾向于读取最近读取数据附近的数据的应用程序有用（例如，顺序读取，或同一局部性组中不同列的随机读取）。

### 6.4 Bloom过滤器

如第5.3节所述，读操作必须从组成tablet状态的所有SSTable中读取。如果这些SSTable不在内存中，我们最终可能会进行许多磁盘访问。我们通过允许客户端指定应该为特定局部性组的SSTable创建Bloom过滤器来减少访问次数。Bloom过滤器允许我们询问SSTable是否可能包含特定行/列对的任何数据。对于某些应用程序，我们使用Bloom过滤器大大减少了读操作所需的磁盘搜索次数。使用Bloom过滤器意味着对不存在行的大多数查找不需要任何磁盘搜索。

### 6.5 提交日志实现

如果我们为每个tablet保留单独的日志文件，则会同时写入大量文件到GFS。根据每个GFS服务器的底层文件系统实现，这些写入可能导致大量磁盘搜索以在不同的物理日志文件之间写入。此外，每个tablet的单独日志文件还会降低组提交优化的有效性，因为组会更小。为了解决这些问题，我们将每个tablet服务器的突变附加到单个提交日志，将不同tablet的突变混合在同一物理日志文件中。

使用一个日志为正常操作提供了显著的性能优势，但它使恢复复杂化。当tablet服务器死亡时，它所服务的tablet将移动到大量其他tablet服务器：每个服务器通常加载原始服务器的少数tablet。为了恢复tablet的状态，新的tablet服务器需要从原始服务器写入的提交日志中重新应用该tablet的突变。但是，这些tablet的突变混合在同一物理日志文件中。一种方法是让每个新的tablet服务器读取完整的提交日志文件并仅应用它需要恢复的tablet的条目。但是，在这种方法下，如果100台机器每台都被分配了失败的tablet服务器的单个tablet，则日志文件将被读取100次（每个服务器一次）。

我们通过首先将提交日志条目按键（table, row name, log sequence number）排序来避免重复读取日志文件。在排序输出中，特定tablet的所有突变都是连续的，因此可以通过一次磁盘搜索和顺序读取高效读取。为了并行化排序，我们将日志文件分区为64 MB段，并在不同服务器上并行排序每个段。此排序过程由主服务器协调，并在tablet服务器指示它需要从某些提交日志文件中恢复突变时启动。

在将提交日志写入GFS时可能会遇到各种故障。例如，写入可能在写入过程中发生，或者GFS服务器可能遭受故障。每个这样的故障都可能导致日志记录的区域不一致和重复。为了确保恢复代码忽略这些有问题的区域，我们在提交日志记录中包含校验和，以便我们可以识别和跳过这些有问题的区域。

### 6.6 加速tablet恢复

如果主服务器将tablet从一个tablet服务器移动到另一个，源tablet服务器首先对该tablet进行次要压缩。此压缩减少了tablet服务器提交日志中未压缩状态的数量，从而减少了恢复时间。完成此压缩后，tablet服务器停止为tablet提供服务。在实际卸载tablet之前，tablet服务器进行另一次（通常非常快）次要压缩，以消除第一次次要压缩期间到达的任何剩余未压缩状态。完成第二次次要压缩后，可以将tablet加载到另一个tablet服务器上而无需恢复任何日志条目。

### 6.7 利用不变性

除了SSTable缓存之外，BigTable系统的各个其他部分都通过SSTable不变的事实得到了简化。例如，我们不需要在从SSTable读取时同步对文件系统的访问。因此，对行的并发控制可以非常高效地实现。唯一可变的数据结构是memtable，它被频繁读取和写入，是行级写时复制的。由于memtable是有序的数据结构，我们可以高效地实现行级写时复制。

SSTable的不变性使我们能够永久地拆分tablet。我们不是为每个子tablet生成新的SSTable集合，而是让子tablet共享父tablet的SSTable。

## 7. 性能评估

我们建立了一个BigTable集群，其中包含N个tablet服务器，以测量BigTable的性能和可扩展性。（由于各种原因，我们无法发布我们机器的详细信息或完整基准。）每台机器有两个双核Opteron 2 GHz芯片、足够的内存来保存工作集，以及一个IDE硬盘驱动器。N个tablet服务器（和主服务器）放置在N台机器上。客户端在与tablet服务器不同的机器上运行，以确保客户端不会成为性能瓶颈。

我们使用了一个简化版本的标准数据库基准。在基准中，每一行有1000列，每个值是1000字节，因此每一行有大约1 MB的数据。我们报告了几种不同类型的基准的结果。

### 7.1 单tablet服务器性能

我们首先测量了单个tablet服务器的性能。顺序写入以1200 MB/s的速度执行，随机写入以300 MB/s的速度执行，顺序读取以1300 MB/s的速度执行，随机读取以100 MB/s的速度执行。

随机读取比其他操作慢得多，因为每个1000字节的读取都需要从SSTable索引层次结构的根进行64 KB块传输，其中只使用了一小部分。

### 7.2 扩展

图8显示了随着我们增加集群中tablet服务器数量时聚合吞吐量如何变化。随机读取显示出最差的扩展性（在500个tablet服务器上增加了100倍）。这种扩展性的缺乏是由于我们基准测试中每个1000字节读取的开销，该开销在网络堆栈、SSTable索引和BigTable代码路径中传输64KB块。这个问题将在真实应用程序中不太严重，因为它们通常具有更大的值。

随机和顺序写入的扩展性比随机读取更好。它们在500个tablet服务器上的性能比在1个tablet服务器上好约300倍。性能随着tablet服务器数量的增加而下降，因为我们的基准测试客户端只能生成有限的负载。

顺序读取显示出最好的扩展性，在500个tablet服务器上的性能比在1个tablet服务器上好近600倍。这种出色的扩展性发生是因为每个基准测试读取大量数据，因此传输64KB SSTable块的开销被摊销在大量有用数据上。

## 8. 真实应用

到2006年8月，有388个非测试BigTable集群在各种Google机器集群中运行，总共约24,500个tablet服务器。表1显示了一些集群的粗略分布。许多这些集群用于开发目的，因此在任何给定时间都处于空闲状态。一个繁忙集群的聚合吞吐量超过1200 MB/s。

### 8.1 Google Analytics

Google Analytics是一项服务，帮助网站管理员分析其网站的流量模式。它为网站管理员提供聚合统计信息，如每天唯一访问者数量、每天每个URL的页面浏览量以及显示用户访问模式的图形。

为了启用Google Analytics服务，网站管理员在他们想要监控的网页上嵌入一小段JavaScript代码。每当访问这样的网页时，这段代码就会被激活。它记录各种信息，如用户标识符和页面信息，发送给Google Analytics，Google Analytics汇总这些信息并将其呈现给网站管理员。

我们简要描述Google Analytics使用的两个表。

原始点击表（~200 TB）为每个最终用户会话维护一行。行名是网站名称和会话创建时间的元组。这种模式确保访问网站的会话按时间顺序连续。该表压缩到其原始大小的14%。

汇总表（~20 TB）包含每个网站的各种预定义汇总。该表通过定期MapReduce作业从原始点击表生成，这些作业从原始点击表中提取最近的会话数据。系统的整体吞吐量受到GFS吞吐量的限制。该表压缩到其原始大小的29%。

### 8.2 Google Earth

Google为用户提供高分辨率的地球卫星图像，可通过基于Web的Google Maps界面（maps.google.com）和Google Earth客户端软件（earth.google.com）访问。这些产品允许用户在任何分辨率下浏览卫星图像，平移和缩放，并注释图像。

该系统使用一个表进行预处理，使用几个表来服务客户端数据。预处理管道使用一个表来存储原始图像。在预处理过程中，图像被清理和合并成最终的可服务数据。该表存储大约70 TB的数据，因此保存在磁盘上。图像已经高效压缩，因此BigTable压缩被禁用。

表中的每一行代表单个地理段。行被命名，以便地理上相邻的段存储在一起。该表包含一个列族，用于跟踪每个段的数据源。该列族有大量列：基本上每个原始数据图像一列。由于每个段仅由少数图像构建，因此该列族非常稀疏。

预处理管道严重依赖MapReduce来转换BigTable中的数据。在某些MapReduce作业期间，系统处理数据的速度超过每个tablet服务器1 MB/s。

服务系统使用一个表来索引存储在GFS中的数据。该表相对较小（~500 GB），但必须在每个数据中心每秒数万次查询的负载下提供低延迟。因此，该表分布在数百个tablet服务器上，并包含内存中的列族。

### 8.3 个性化搜索

个性化搜索（www.google.com/psearch）是一项可选服务，记录用户查询和在各种Google属性（如网络搜索、图像和新闻）上的点击。用户可以浏览其搜索历史以查看其查询和点击，并且可以要求根据其历史Google使用模式获得个性化搜索结果。

个性化搜索将每个用户的数据存储在BigTable中。每个用户都有一个唯一的用户ID，并被分配一个以该用户ID命名的行。所有用户操作都存储在一个表中，每种类型的操作都有自己的列族（例如，有一个列族用于所有Web查询）。每个数据项都使用操作发生的时间作为其BigTable时间戳。

个性化搜索使用MapReduce在BigTable上生成用户配置文件。这些用户配置文件用于个性化实时搜索结果。

个性化搜索数据在多个BigTable集群之间复制，以增加可用性并减少由于距离引起的客户端延迟。个性化搜索团队最初在BigTable之上开发了自己的客户端复制机制，以确保所有副本的最终一致性。现在复制子系统已集成到服务器中。

个性化搜索存储系统的设计允许其他组在其各自的列中添加新的每用户信息，现在许多其他Google属性使用它来存储每用户配置选项和设置。在多个产品团队之间共享表导致了异常大量的列族。为了帮助支持共享，我们为BigTable添加了一个简单的配额机制，限制任何单个客户端在共享表中可以消耗多少存储空间。该机制为使用BigTable存储每用户信息的多个产品团队提供了一定程度的隔离。

## 9. 经验教训

在设计、实现、维护和支持BigTable的过程中，我们获得了有用的经验并学到了几个有趣的教训。

### 9.1 故障源比预期更多

大型分布式系统容易受到许多类型故障的影响，不仅仅是许多分布式协议假设的标准网络分区和故障停止故障。我们看到了由以下原因引起的问题：

- 内存和网络损坏
- 大时钟偏差
- 机器挂起
- 扩展和不对称网络分区
- 我们使用的服务中的错误（例如Chubby）
- GFS配额溢出
- 计划和非计划硬件维护

随着我们对这些问题的经验增加，我们通过更改各种协议来解决它们。例如，我们向RPC机制添加了校验和。我们还通过删除系统一部分对另一部分的假设来处理一些问题。例如，我们停止假设给定的Chubby操作只返回一组固定的错误。

### 9.2 避免添加使用不明确的功能

避免添加新功能直到清楚如何使用它们是重要的。例如，我们最初计划在我们的API中支持通用事务。因为我们没有立即的用例，我们没有实现它们。现在我们有许多真实的应用程序在BigTable上运行，我们已经能够检查它们的实际需求，并发现大多数应用程序只需要单行事务。在需要分布式事务的地方，最重要的用途是维护辅助索引，我们计划添加一个专门的机制来满足这种需求。这种机制将不如通用事务通用，但将更高效（特别是对于跨数百行的更新）并且将与我们的系统更好地集成以进行乐观跨数据中心复制。

### 9.3 系统级监控的重要性

在支持BigTable的实际经验中学到的一个重要教训是适当的系统级监控的价值（即，监控BigTable本身以及使用BigTable的客户端）。例如，我们扩展了RPC系统以保持重要操作的详细跟踪。此功能帮助我们检测和修复许多问题，例如：

- tablet数据结构上的锁争用
- 在提交BigTable突变时GFS写入缓慢
- 当METADATA tablet不可用时访问METADATA表被卡住

监控的另一个示例是每个BigTable集群都在Chubby中注册。这使我们能够跟踪所有集群，发现它们有多大，查看它们运行的软件版本，监控它们接收多少流量，以及查看是否存在任何异常大的延迟。

### 9.4 简单设计的价值

我们学到的最重要的教训之一是简单设计的价值。考虑到我们系统的规模（大约100,000行非测试代码）以及代码随时间以意想不到的方式演变的事实，我们发现代码和设计清晰对代码维护和调试有巨大帮助。

一个例子是我们的tablet服务器成员协议。我们的第一个协议很简单：主服务器定期向每个tablet服务器发出租约，如果tablet服务器的租约到期，它会自杀。不幸的是，这个协议在存在网络问题时可用性很差，并且对主恢复时间敏感。我们重新设计了协议几次，直到我们有了令人满意的东西。但是，协议变得过于复杂，并且依赖于很少被其他应用程序使用的Chubby功能。我们发现我们花费了大量时间调试奇怪的边缘情况，不仅在BigTable代码中，而且在Chubby代码中。最终，我们丢弃了该协议，转向了一个更简单的协议，该协议仅依赖于广泛使用的Chubby功能。

## 10. 相关工作

Boxwood项目的组件在某些方面与Chubby、GFS和BigTable重叠，因为它提供分布式一致性、锁定、分布式块存储和分布式B树存储。在每种重叠的情况下，Boxwood的组件似乎针对比相应Google服务稍低的级别。Boxwood项目的目标是为构建更高级服务（如文件系统或数据库）提供基础设施，而BigTable的目标是直接支持希望存储数据的客户端应用程序。

许多最近的项目已经解决了在广域网络上提供分布式存储或更高级服务的问题，通常在"互联网规模"。这包括从CAN、Chord、Tapestry和Pastry等项目开始的分布式哈希表工作。这些系统解决了BigTable不会出现的问题，如高度可变的带宽、不受信任的参与者或频繁的重新配置；去中心化控制和拜占庭容错不是BigTable目标。

就可能向应用程序开发人员提供的分布式数据存储模型而言，我们认为分布式B树或分布式哈希表提供的键值对模型过于限制。键值对是有用的构建块，但它们不应该是向开发人员提供的唯一构建块。我们选择的模型比简单的键值对更丰富，并支持稀疏半结构化数据。尽管如此，它仍然足够简单，适合非常高效的表示，并且足够透明（通过局部性组）以允许我们的用户调整系统的重要行为。

几个数据库供应商开发了可以存储大量数据的并行数据库。Oracle的Real Application Cluster数据库使用共享磁盘存储数据（BigTable使用GFS）和分布式锁管理器（BigTable使用Chubby）。IBM的DB2 Parallel Edition基于类似于BigTable的无共享架构。每个DB2服务器负责表中行的子集，它将其存储在本地关系数据库中。两种产品都提供具有事务的完整关系模型。

BigTable局部性组实现了为其他系统观察到的类似压缩和磁盘读取性能优势，这些系统使用基于列而不是基于行的存储在磁盘上组织数据，包括C-Store和商业产品，如Sybase IQ、SenSage、KDB+和MonetDB/X100中的ColumnBM存储层。另一个进行垂直和水平数据分区并实现良好数据压缩比的系统是AT&T的Daytona数据库。局部性组不支持CPU缓存级优化，如Ailamaki描述的那些。

BigTable使用memtable和SSTable存储tablet更新的方式类似于Log-Structured Merge Tree存储索引数据更新的方式。在两个系统中，排序数据在写入磁盘之前在内存中缓冲，读取必须合并来自内存和磁盘的数据。

C-Store和BigTable共享许多特征：两个系统都使用无共享架构，并有两种不同的数据结构，一种用于最近的写入，一种用于存储长期数据，以及将数据从一种形式移动到另一种形式的机制。系统在其API方面显著不同：C-Store的行为类似于关系数据库，而BigTable提供较低级别的读写接口，并设计为支持每个服务器每秒数千次此类操作。C-Store也是"读优化关系DBMS"，而BigTable在读密集型和写密集型应用程序上都提供良好的性能。

BigTable的负载平衡器必须解决一些与无共享数据库面临的相同类型的负载和内存平衡问题。我们的问题有些简单：(1) 我们不考虑同一数据的多个副本的可能性，可能由于视图或索引而以替代形式存在；(2) 我们让用户告诉我们哪些数据属于内存，哪些数据应该保留在磁盘上，而不是尝试动态确定这一点；(3) 我们没有复杂的查询要执行或优化。

## 11. 结论

我们描述了BigTable，Google的结构化数据分布式存储系统。BigTable自2005年4月以来一直在生产中使用，在此之前我们在设计和实现上花费了大约七个人年。截至2006年8月，超过六十个项目正在使用BigTable。我们的用户喜欢BigTable提供的性能和高可用性，他们可以通过简单地向系统添加更多机器来扩展其BigTable集群的容量，因为负载增加。

鉴于BigTable的不寻常接口，一个有趣的问题是我们的用户适应使用BigTable有多困难。新用户有时不确定如何最好地使用BigTable接口，特别是如果他们习惯于使用支持通用事务的关系数据库。尽管如此，许多Google产品成功使用BigTable的事实表明我们的设计在实践中运行良好。

我们目前正在添加几个新功能，如支持辅助索引和构建具有跨数据中心复制的多主BigTable的基础设施。我们还开始将BigTable作为服务部署给产品组，以便各个组不需要维护自己的集群。随着我们的服务集群扩展，我们将需要处理BigTable内更多的资源共享问题。

最后，我们发现构建我们自己的存储解决方案带来了显著的优势。我们为BigTable设计自己的数据模型已经给了我们很大的灵活性。此外，我们对BigTable实现以及BigTable依赖的其他Google基础设施的控制意味着我们可以在出现瓶颈或效率低下时解决它们。

---

## 参考文献

本文翻译自Google 2006年发表的经典论文《Bigtable: A Distributed Storage System for Structured Data》。

**原文链接**: [Google Research](https://research.google.com/archive/bigtable.html)

**中文翻译参考**: [ArthurChiao's Blog](https://arthurchiao.github.io/blog/google-bigtable-zh/)

---

*本文是Google大数据三篇经典论文系列的第三篇，前两篇分别是《MapReduce: 大型集群上的简化数据处理》和《Google文件系统》。这三篇论文奠定了现代大数据处理和分布式存储系统的理论基础，对整个行业产生了深远影响。*